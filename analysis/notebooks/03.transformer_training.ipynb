{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Training for Relay Optimization\n",
    "\n",
    "This notebook trains a transformer network with Optuna to learn from relay optimization data and generalize optimal values.\n",
    "\n",
    "## Objectives:\n",
    "1. Load existing GA optimization data\n",
    "2. Train a transformer to predict optimal TDS and pickup values\n",
    "3. Use Optuna to optimize model hyperparameters\n",
    "4. Generalize optimization values for new scenarios\n",
    "\n",
    "**üöÄ SINGLE CELL EXECUTION - Run All Button Compatible**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-09 17:13:05,617] A new study created in memory with name: no-name-7e66e918-c70e-47e9-acef-c40302dc5635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ TRANSFORMER TRAINING - COMPLETE EXECUTION\n",
      "============================================================\n",
      "Using device: cpu\n",
      "üìÅ Model directory: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer\n",
      "üìÇ Data paths configured\n",
      "‚úÖ Transformer model architecture defined\n",
      "üîÑ Loading and processing data...\n",
      "üìä Raw data loaded: 6800 relay pairs\n",
      "üìä GA results loaded: 68 scenarios\n",
      "üìä Training dataset created: 6732 samples\n",
      "üìä Scenarios included: 68\n",
      "üîÑ Preparing training data...\n",
      "üìä Training samples: 5385\n",
      "üìä Validation samples: 1347\n",
      "üìä Input features: 6\n",
      "üìä Output features: 4\n",
      "‚úÖ Training functions defined\n",
      "üîÑ Starting Optuna optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-09 17:13:14,221] Trial 0 finished with value: 0.6408095047917477 and parameters: {'d_model': 64, 'nhead': 4, 'num_encoder_layers': 3, 'dim_feedforward': 512, 'dropout': 0.11441735456773089, 'learning_rate': 0.0004799477427860132, 'batch_size': 32, 'weight_decay': 8.684192176131874e-05}. Best is trial 0 with value: 0.6408095047917477.\n",
      "[I 2025-10-09 17:13:35,624] Trial 1 finished with value: 0.766578584208208 and parameters: {'d_model': 128, 'nhead': 8, 'num_encoder_layers': 5, 'dim_feedforward': 512, 'dropout': 0.1835533279885399, 'learning_rate': 0.0008709538316185319, 'batch_size': 16, 'weight_decay': 2.8242272583931502e-05}. Best is trial 0 with value: 0.6408095047917477.\n",
      "[I 2025-10-09 17:13:53,619] Trial 2 finished with value: 0.6342564634112424 and parameters: {'d_model': 32, 'nhead': 8, 'num_encoder_layers': 4, 'dim_feedforward': 512, 'dropout': 0.16293126217841666, 'learning_rate': 0.0003184307682466833, 'batch_size': 32, 'weight_decay': 2.1404183996371762e-05}. Best is trial 2 with value: 0.6342564634112424.\n",
      "[I 2025-10-09 17:14:33,565] Trial 3 finished with value: 0.6494306554627973 and parameters: {'d_model': 64, 'nhead': 4, 'num_encoder_layers': 6, 'dim_feedforward': 1024, 'dropout': 0.15951821560581525, 'learning_rate': 2.5579855318679666e-05, 'batch_size': 32, 'weight_decay': 2.6065494840864308e-05}. Best is trial 2 with value: 0.6342564634112424.\n",
      "[I 2025-10-09 17:14:48,255] Trial 4 finished with value: 0.640278113442798 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 3, 'dim_feedforward': 256, 'dropout': 0.12845869683244446, 'learning_rate': 5.676464979600582e-05, 'batch_size': 32, 'weight_decay': 4.038336752736176e-05}. Best is trial 2 with value: 0.6342564634112424.\n",
      "[I 2025-10-09 17:14:53,451] Trial 5 pruned. \n",
      "[I 2025-10-09 17:15:02,321] Trial 6 finished with value: 0.6388279904018749 and parameters: {'d_model': 32, 'nhead': 16, 'num_encoder_layers': 3, 'dim_feedforward': 512, 'dropout': 0.1500537776031479, 'learning_rate': 0.00040635541451217383, 'batch_size': 64, 'weight_decay': 1.1289862615218072e-06}. Best is trial 2 with value: 0.6342564634112424.\n",
      "[I 2025-10-09 17:15:20,198] Trial 7 finished with value: 0.6339835528065176 and parameters: {'d_model': 32, 'nhead': 4, 'num_encoder_layers': 2, 'dim_feedforward': 256, 'dropout': 0.15608834681762324, 'learning_rate': 0.0009001885345573727, 'batch_size': 16, 'weight_decay': 7.63241306734091e-06}. Best is trial 7 with value: 0.6339835528065176.\n",
      "[I 2025-10-09 17:15:49,007] Trial 8 pruned. \n",
      "[I 2025-10-09 17:16:02,831] Trial 9 finished with value: 0.6429854153200637 and parameters: {'d_model': 128, 'nhead': 16, 'num_encoder_layers': 3, 'dim_feedforward': 512, 'dropout': 0.18633668447025903, 'learning_rate': 2.763108404999945e-05, 'batch_size': 32, 'weight_decay': 1.8676942318586863e-06}. Best is trial 7 with value: 0.6339835528065176.\n",
      "[I 2025-10-09 17:16:14,566] Trial 10 finished with value: 0.6479383538751041 and parameters: {'d_model': 32, 'nhead': 4, 'num_encoder_layers': 2, 'dim_feedforward': 1024, 'dropout': 0.2536308891188272, 'learning_rate': 0.0009904566510682345, 'batch_size': 16, 'weight_decay': 6.366166026444662e-06}. Best is trial 7 with value: 0.6339835528065176.\n",
      "[I 2025-10-09 17:16:34,459] Trial 11 finished with value: 0.6336612112381879 and parameters: {'d_model': 32, 'nhead': 8, 'num_encoder_layers': 2, 'dim_feedforward': 256, 'dropout': 0.22144075831382581, 'learning_rate': 0.00028388872855297846, 'batch_size': 16, 'weight_decay': 1.2452057429538313e-05}. Best is trial 11 with value: 0.6336612112381879.\n",
      "[I 2025-10-09 17:16:45,423] Trial 12 pruned. \n",
      "[I 2025-10-09 17:17:00,686] Trial 13 pruned. \n",
      "[I 2025-10-09 17:17:19,021] Trial 14 finished with value: 0.632614888864405 and parameters: {'d_model': 32, 'nhead': 16, 'num_encoder_layers': 2, 'dim_feedforward': 256, 'dropout': 0.20647313147231663, 'learning_rate': 0.000581065610494624, 'batch_size': 16, 'weight_decay': 3.185175750821139e-06}. Best is trial 14 with value: 0.632614888864405.\n",
      "[I 2025-10-09 17:17:28,629] Trial 15 pruned. \n",
      "[I 2025-10-09 17:17:48,557] Trial 16 pruned. \n",
      "[I 2025-10-09 17:17:57,323] Trial 17 pruned. \n",
      "[I 2025-10-09 17:18:19,714] Trial 18 pruned. \n",
      "[I 2025-10-09 17:18:34,429] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Optuna optimization completed!\n",
      "üìä Best trial: 14\n",
      "üìä Best validation loss: 0.632615\n",
      "üìä Best parameters: {'d_model': 32, 'nhead': 16, 'num_encoder_layers': 2, 'dim_feedforward': 256, 'dropout': 0.20647313147231663, 'learning_rate': 0.000581065610494624, 'batch_size': 16, 'weight_decay': 3.185175750821139e-06}\n",
      "üîÑ Training final model with best parameters...\n",
      "üîÑ Training final model...\n",
      "   Epoch   0: Train Loss = 0.774303, Val Loss = 0.677476\n",
      "   Epoch   5: Train Loss = 0.646171, Val Loss = 0.641147\n",
      "   Epoch  10: Train Loss = 0.635420, Val Loss = 0.648110\n",
      "   Epoch  15: Train Loss = 0.632565, Val Loss = 0.639873\n",
      "   Epoch  20: Train Loss = 0.629637, Val Loss = 0.637422\n",
      "   Epoch  25: Train Loss = 0.628015, Val Loss = 0.647966\n",
      "   Epoch  30: Train Loss = 0.626402, Val Loss = 0.645309\n",
      "   Epoch  35: Train Loss = 0.624633, Val Loss = 0.634499\n",
      "   Early stopping at epoch 37\n",
      "‚úÖ Final model training completed!\n",
      "üìä Best validation loss: 0.633212\n",
      "üìä Total epochs: 38\n",
      "üîÑ Saving model and artifacts...\n",
      "‚úÖ Model and artifacts saved!\n",
      "üìÅ Model file: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/best_relay_optimization_transformer.pth\n",
      "üìÅ Scaler input: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/scaler_input.pkl\n",
      "üìÅ Scaler target: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/scaler_target.pkl\n",
      "üìÅ Best params: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/best_params.json\n",
      "üìÅ Training summary: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/training_summary.json\n",
      "üîÑ Evaluating final model...\n",
      "üìä Final Model Performance:\n",
      "   ‚Ä¢ MSE: 0.044420\n",
      "   ‚Ä¢ MAE: 0.141402\n",
      "   ‚Ä¢ R¬≤: 0.350989\n",
      "   ‚Ä¢ RMSE: 0.210761\n",
      "‚úÖ Predictor class saved: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/transformer_predictor.py\n",
      "\n",
      "============================================================\n",
      "üéâ TRANSFORMER TRAINING COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "\n",
      "üìä TRAINING SUMMARY:\n",
      "   ‚Ä¢ Total training samples: 5385\n",
      "   ‚Ä¢ Total validation samples: 1347\n",
      "   ‚Ä¢ Best validation loss: 0.633212\n",
      "   ‚Ä¢ Final R¬≤ score: 0.350989\n",
      "   ‚Ä¢ Training epochs: 38\n",
      "\n",
      "üìÅ FILES CREATED:\n",
      "   ‚Ä¢ Model: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/best_relay_optimization_transformer.pth\n",
      "   ‚Ä¢ Input scaler: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/scaler_input.pkl\n",
      "   ‚Ä¢ Target scaler: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/scaler_target.pkl\n",
      "   ‚Ä¢ Best parameters: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/best_params.json\n",
      "   ‚Ä¢ Training summary: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/training_summary.json\n",
      "   ‚Ä¢ Predictor class: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/transformer_predictor.py\n",
      "\n",
      "üöÄ NEXT STEPS:\n",
      "   1. Run the validation notebook to test the model\n",
      "   2. Use the predictor class for new relay optimizations\n",
      "   3. Deploy the model for production use\n",
      "\n",
      "‚úÖ All operations completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TRANSFORMER TRAINING FOR RELAY OPTIMIZATION - COMPLETE EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import math\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üöÄ TRANSFORMER TRAINING - COMPLETE EXECUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION AND PATHS\n",
    "# =============================================================================\n",
    "\n",
    "PROJECT_ROOT = Path(\"/Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG\")\n",
    "MODEL_DIR = PROJECT_ROOT / \"models\" / \"transformer\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RAW_DATA_PATH = PROJECT_ROOT / \"data\" / \"raw\" / \"automation_results.json\"\n",
    "OPTIMIZATION_RESULTS_PATH = PROJECT_ROOT / \"data\" / \"processed\" / \"ga_optimization_all_scenarios_comprehensive_20251008_224215.json\"\n",
    "\n",
    "# Model files\n",
    "MODEL_PATH = MODEL_DIR / \"best_relay_optimization_transformer.pth\"\n",
    "SCALER_INPUT_PATH = MODEL_DIR / \"scaler_input.pkl\"\n",
    "SCALER_TARGET_PATH = MODEL_DIR / \"scaler_target.pkl\"\n",
    "BEST_PARAMS_PATH = MODEL_DIR / \"best_params.json\"\n",
    "TRAINING_SUMMARY_PATH = MODEL_DIR / \"training_summary.json\"\n",
    "\n",
    "print(f\"üìÅ Model directory: {MODEL_DIR}\")\n",
    "print(f\"üìÇ Data paths configured\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRANSFORMER MODEL ARCHITECTURE\n",
    "# =============================================================================\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class RelayOptimizationTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(RelayOptimizationTransformer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.output_proj = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.input_proj.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output_proj.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Asegurar que src tenga 3 dimensiones (batch_size, sequence_length, features)\n",
    "        if src.dim() == 2:\n",
    "            # Si src tiene 2 dimensiones, agregar una dimensi√≥n de secuencia\n",
    "            src = src.unsqueeze(1)  # (batch_size, 1, features)\n",
    "        \n",
    "        src = self.input_proj(src) * math.sqrt(self.d_model)\n",
    "        # Transformer espera (sequence_length, batch_size, features)\n",
    "        src = src.permute(1, 0, 2)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        # Volver a (batch_size, sequence_length, features)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        # Promedio sobre la secuencia para obtener (batch_size, features)\n",
    "        output = output.mean(dim=1)\n",
    "        output = self.output_proj(output)\n",
    "        return output\n",
    "\n",
    "print(\"‚úÖ Transformer model architecture defined\")\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_process_data():\n",
    "    \"\"\"\n",
    "    Carga y procesa los datos de rel√©s y optimizaci√≥n GA\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Loading and processing data...\")\n",
    "    \n",
    "    # Cargar datos originales\n",
    "    with open(RAW_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    # Cargar resultados de optimizaci√≥n GA\n",
    "    with open(OPTIMIZATION_RESULTS_PATH, 'r', encoding='utf-8') as f:\n",
    "        ga_results = json.load(f)\n",
    "    \n",
    "    print(f\"üìä Raw data loaded: {len(raw_data)} relay pairs\")\n",
    "    print(f\"üìä GA results loaded: {len(ga_results['optimization_results'])} scenarios\")\n",
    "    \n",
    "    # Agrupar datos originales por escenario\n",
    "    raw_by_scenario = defaultdict(list)\n",
    "    for entry in raw_data:\n",
    "        raw_by_scenario[entry['scenario_id']].append(entry)\n",
    "    \n",
    "    # Obtener resultados GA por escenario\n",
    "    ga_by_scenario = ga_results['optimization_results']\n",
    "    \n",
    "    # Crear dataset de entrenamiento\n",
    "    training_data = []\n",
    "    \n",
    "    for scenario_id in ga_by_scenario.keys():\n",
    "        if scenario_id not in raw_by_scenario:\n",
    "            continue\n",
    "            \n",
    "        scenario_raw = raw_by_scenario[scenario_id]\n",
    "        scenario_ga = ga_by_scenario[scenario_id]\n",
    "        \n",
    "        optimized_relays = scenario_ga['relay_values']\n",
    "        \n",
    "        for relay_pair in scenario_raw:\n",
    "            main_relay_id = relay_pair['main_relay']['relay']\n",
    "            backup_relay_id = relay_pair['backup_relay']['relay']\n",
    "            \n",
    "            # Verificar si ambos rel√©s fueron optimizados\n",
    "            if main_relay_id in optimized_relays and backup_relay_id in optimized_relays:\n",
    "                \n",
    "                # Caracter√≠sticas de entrada\n",
    "                input_features = [\n",
    "                    float(relay_pair['fault']),\n",
    "                    relay_pair['main_relay']['Ishc'],\n",
    "                    relay_pair['main_relay']['Time_out'],\n",
    "                    relay_pair['backup_relay']['Ishc'],\n",
    "                    relay_pair['backup_relay']['Time_out'],\n",
    "                    len(scenario_raw)\n",
    "                ]\n",
    "                \n",
    "                # Caracter√≠sticas objetivo (valores optimizados por GA)\n",
    "                target_features = [\n",
    "                    optimized_relays[main_relay_id]['TDS'],\n",
    "                    optimized_relays[main_relay_id]['pickup'],\n",
    "                    optimized_relays[backup_relay_id]['TDS'],\n",
    "                    optimized_relays[backup_relay_id]['pickup']\n",
    "                ]\n",
    "                \n",
    "                training_data.append({\n",
    "                    'input': input_features,\n",
    "                    'target': target_features,\n",
    "                    'scenario_id': scenario_id,\n",
    "                    'main_relay': main_relay_id,\n",
    "                    'backup_relay': backup_relay_id\n",
    "                })\n",
    "    \n",
    "    print(f\"üìä Training dataset created: {len(training_data)} samples\")\n",
    "    print(f\"üìä Scenarios included: {len(set(d['scenario_id'] for d in training_data))}\")\n",
    "    \n",
    "    if len(training_data) == 0:\n",
    "        raise ValueError(\"No training data created. Check data files.\")\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "# Cargar datos\n",
    "training_data = load_and_process_data()\n",
    "\n",
    "# =============================================================================\n",
    "# DATA PREPARATION FOR TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_training_data(training_data):\n",
    "    \"\"\"\n",
    "    Prepara los datos para el entrenamiento\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Preparing training data...\")\n",
    "    \n",
    "    # Extraer caracter√≠sticas de entrada y objetivo\n",
    "    X = np.array([item['input'] for item in training_data])\n",
    "    y = np.array([item['target'] for item in training_data])\n",
    "    \n",
    "    # Dividir en entrenamiento y validaci√≥n\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Normalizar caracter√≠sticas de entrada\n",
    "    scaler_input = StandardScaler()\n",
    "    X_train_scaled = scaler_input.fit_transform(X_train)\n",
    "    X_val_scaled = scaler_input.transform(X_val)\n",
    "    \n",
    "    # Normalizar caracter√≠sticas objetivo\n",
    "    scaler_target = StandardScaler()\n",
    "    y_train_scaled = scaler_target.fit_transform(y_train)\n",
    "    y_val_scaled = scaler_target.transform(y_val)\n",
    "    \n",
    "    print(f\"üìä Training samples: {len(X_train_scaled)}\")\n",
    "    print(f\"üìä Validation samples: {len(X_val_scaled)}\")\n",
    "    print(f\"üìä Input features: {X_train_scaled.shape[1]}\")\n",
    "    print(f\"üìä Output features: {y_train_scaled.shape[1]}\")\n",
    "    \n",
    "    return (X_train_scaled, X_val_scaled, y_train_scaled, y_val_scaled, \n",
    "            scaler_input, scaler_target)\n",
    "\n",
    "# Preparar datos\n",
    "X_train, X_val, y_train, y_val, scaler_input, scaler_target = prepare_training_data(training_data)\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Entrena el modelo por una √©poca\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in dataloader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Valida el modelo por una √©poca\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in dataloader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Funci√≥n objetivo para Optuna\n",
    "    \"\"\"\n",
    "    # Par√°metros del modelo\n",
    "    d_model = trial.suggest_categorical('d_model', [32, 64, 128])\n",
    "    nhead = trial.suggest_categorical('nhead', [4, 8, 16])\n",
    "    num_encoder_layers = trial.suggest_int('num_encoder_layers', 2, 6)\n",
    "    dim_feedforward = trial.suggest_categorical('dim_feedforward', [256, 512, 1024])\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.3)\n",
    "    \n",
    "    # Par√°metros de entrenamiento\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-4, log=True)\n",
    "    \n",
    "    # Crear modelo\n",
    "    model = RelayOptimizationTransformer(\n",
    "        input_dim=6,\n",
    "        output_dim=4,\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizador y funci√≥n de p√©rdida\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Crear dataloaders\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.float32)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(20):  # M√°ximo 20 √©pocas por trial\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "        \n",
    "        # Reportar m√©tricas intermedias\n",
    "        trial.report(val_loss, epoch)\n",
    "        \n",
    "        # Verificar si el trial debe ser podado\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return best_val_loss\n",
    "\n",
    "print(\"‚úÖ Training functions defined\")\n",
    "\n",
    "# =============================================================================\n",
    "# OPTUNA OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîÑ Starting Optuna optimization...\")\n",
    "\n",
    "# Crear estudio Optuna\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    ")\n",
    "\n",
    "# Ejecutar optimizaci√≥n\n",
    "study.optimize(objective, n_trials=20, timeout=1800)  # 20 trials, 30 min timeout\n",
    "\n",
    "print(f\"‚úÖ Optuna optimization completed!\")\n",
    "print(f\"üìä Best trial: {study.best_trial.number}\")\n",
    "print(f\"üìä Best validation loss: {study.best_value:.6f}\")\n",
    "print(f\"üìä Best parameters: {study.best_params}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL MODEL TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîÑ Training final model with best parameters...\")\n",
    "\n",
    "# Obtener mejores par√°metros\n",
    "best_params = study.best_params\n",
    "\n",
    "# Crear modelo final\n",
    "final_model = RelayOptimizationTransformer(\n",
    "    input_dim=6,\n",
    "    output_dim=4,\n",
    "    d_model=best_params['d_model'],\n",
    "    nhead=best_params['nhead'],\n",
    "    num_encoder_layers=best_params['num_encoder_layers'],\n",
    "    dim_feedforward=best_params['dim_feedforward'],\n",
    "    dropout=best_params['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Optimizador y funci√≥n de p√©rdida\n",
    "optimizer = optim.Adam(\n",
    "    final_model.parameters(), \n",
    "    lr=best_params['learning_rate'], \n",
    "    weight_decay=best_params['weight_decay']\n",
    ")\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Crear dataloaders\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.float32)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.tensor(X_val, dtype=torch.float32),\n",
    "    torch.tensor(y_val, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "# Entrenar modelo final\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"üîÑ Training final model...\")\n",
    "\n",
    "for epoch in range(50):  # M√°ximo 50 √©pocas\n",
    "    train_loss = train_epoch(final_model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = validate_epoch(final_model, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"   Epoch {epoch:3d}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Guardar mejor modelo\n",
    "        torch.save(final_model.state_dict(), MODEL_PATH)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"   Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "print(f\"‚úÖ Final model training completed!\")\n",
    "print(f\"üìä Best validation loss: {best_val_loss:.6f}\")\n",
    "print(f\"üìä Total epochs: {len(train_losses)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE MODEL AND ARTIFACTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîÑ Saving model and artifacts...\")\n",
    "\n",
    "# Guardar scalers\n",
    "with open(SCALER_INPUT_PATH, 'wb') as f:\n",
    "    pickle.dump(scaler_input, f)\n",
    "\n",
    "with open(SCALER_TARGET_PATH, 'wb') as f:\n",
    "    pickle.dump(scaler_target, f)\n",
    "\n",
    "# Guardar mejores par√°metros\n",
    "with open(BEST_PARAMS_PATH, 'w') as f:\n",
    "    json.dump(best_params, f, indent=4)\n",
    "\n",
    "# Guardar resumen de entrenamiento\n",
    "training_summary = {\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'best_params': best_params,\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'num_final_epochs': len(train_losses),\n",
    "    'training_date': time.strftime('%Y-%m-%dT%H:%M:%S.%f'),\n",
    "    'mode': 'SINGLE_CELL_EXECUTION'\n",
    "}\n",
    "\n",
    "with open(TRAINING_SUMMARY_PATH, 'w') as f:\n",
    "    json.dump(training_summary, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Model and artifacts saved!\")\n",
    "print(f\"üìÅ Model file: {MODEL_PATH}\")\n",
    "print(f\"üìÅ Scaler input: {SCALER_INPUT_PATH}\")\n",
    "print(f\"üìÅ Scaler target: {SCALER_TARGET_PATH}\")\n",
    "print(f\"üìÅ Best params: {BEST_PARAMS_PATH}\")\n",
    "print(f\"üìÅ Training summary: {TRAINING_SUMMARY_PATH}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîÑ Evaluating final model...\")\n",
    "\n",
    "# Cargar mejor modelo\n",
    "final_model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "final_model.eval()\n",
    "\n",
    "# Evaluar en datos de validaci√≥n\n",
    "with torch.no_grad():\n",
    "    val_predictions = final_model(torch.tensor(X_val, dtype=torch.float32).to(device))\n",
    "    val_predictions = val_predictions.cpu().numpy()\n",
    "    \n",
    "    # Desnormalizar predicciones\n",
    "    val_predictions_denorm = scaler_target.inverse_transform(val_predictions)\n",
    "    val_targets_denorm = scaler_target.inverse_transform(y_val)\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    \n",
    "    mse = mean_squared_error(val_targets_denorm, val_predictions_denorm)\n",
    "    mae = mean_absolute_error(val_targets_denorm, val_predictions_denorm)\n",
    "    r2 = r2_score(val_targets_denorm, val_predictions_denorm)\n",
    "    \n",
    "    print(f\"üìä Final Model Performance:\")\n",
    "    print(f\"   ‚Ä¢ MSE: {mse:.6f}\")\n",
    "    print(f\"   ‚Ä¢ MAE: {mae:.6f}\")\n",
    "    print(f\"   ‚Ä¢ R¬≤: {r2:.6f}\")\n",
    "    print(f\"   ‚Ä¢ RMSE: {np.sqrt(mse):.6f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRANSFORMER PREDICTOR CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class RelayOptimizationPredictor:\n",
    "    \"\"\"\n",
    "    Clase para hacer predicciones con el modelo entrenado\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, scaler_input_path, scaler_target_path, best_params_path):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Cargar par√°metros\n",
    "        with open(best_params_path, 'r') as f:\n",
    "            self.best_params = json.load(f)\n",
    "        \n",
    "        # Cargar scalers\n",
    "        with open(scaler_input_path, 'rb') as f:\n",
    "            self.scaler_input = pickle.load(f)\n",
    "        with open(scaler_target_path, 'rb') as f:\n",
    "            self.scaler_target = pickle.load(f)\n",
    "        \n",
    "        # Crear modelo\n",
    "        self.model = RelayOptimizationTransformer(\n",
    "            input_dim=6,\n",
    "            output_dim=4,\n",
    "            d_model=self.best_params['d_model'],\n",
    "            nhead=self.best_params['nhead'],\n",
    "            num_encoder_layers=self.best_params['num_encoder_layers'],\n",
    "            dim_feedforward=self.best_params['dim_feedforward'],\n",
    "            dropout=self.best_params['dropout']\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Cargar pesos\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict_optimization(self, relay_data):\n",
    "        \"\"\"\n",
    "        Predice valores √≥ptimos para un par de rel√©s\n",
    "        \n",
    "        Args:\n",
    "            relay_data: Lista de diccionarios con datos de rel√©s\n",
    "        \n",
    "        Returns:\n",
    "            Lista de predicciones con valores √≥ptimos\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for relay_pair in relay_data:\n",
    "                # Preparar caracter√≠sticas de entrada\n",
    "                input_features = [\n",
    "                    float(relay_pair['fault']),\n",
    "                    relay_pair['main_relay']['Ishc'],\n",
    "                    relay_pair['main_relay']['Time_out'],\n",
    "                    relay_pair['backup_relay']['Ishc'],\n",
    "                    relay_pair['backup_relay']['Time_out'],\n",
    "                    len(relay_data)\n",
    "                ]\n",
    "                \n",
    "                # Normalizar entrada\n",
    "                input_normalized = self.scaler_input.transform([input_features])\n",
    "                \n",
    "                # Convertir a tensor\n",
    "                input_tensor = torch.tensor(input_normalized, dtype=torch.float32).to(self.device)\n",
    "                \n",
    "                # Hacer predicci√≥n\n",
    "                prediction = self.model(input_tensor)\n",
    "                prediction_np = prediction.cpu().numpy().reshape(-1, 4)[0]\n",
    "                \n",
    "                # Desnormalizar predicci√≥n\n",
    "                prediction_denorm = self.scaler_target.inverse_transform([prediction_np])[0]\n",
    "                \n",
    "                # Crear resultado\n",
    "                result = {\n",
    "                    'main_relay': {\n",
    "                        'relay': relay_pair['main_relay']['relay'],\n",
    "                        'TDS': max(0.05, min(0.8, prediction_denorm[0])),\n",
    "                        'pickup': max(0.05, min(2.0, prediction_denorm[1]))\n",
    "                    },\n",
    "                    'backup_relay': {\n",
    "                        'relay': relay_pair['backup_relay']['relay'],\n",
    "                        'TDS': max(0.05, min(0.8, prediction_denorm[2])),\n",
    "                        'pickup': max(0.05, min(2.0, prediction_denorm[3]))\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                predictions.append(result)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Guardar clase predictor\n",
    "predictor_code = f\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class RelayOptimizationTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(RelayOptimizationTransformer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.output_proj = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.input_proj.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output_proj.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Asegurar que src tenga 3 dimensiones (batch_size, sequence_length, features)\n",
    "        if src.dim() == 2:\n",
    "            # Si src tiene 2 dimensiones, agregar una dimensi√≥n de secuencia\n",
    "            src = src.unsqueeze(1)  # (batch_size, 1, features)\n",
    "        \n",
    "        src = self.input_proj(src) * math.sqrt(self.d_model)\n",
    "        # Transformer espera (sequence_length, batch_size, features)\n",
    "        src = src.permute(1, 0, 2)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        # Volver a (batch_size, sequence_length, features)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        # Promedio sobre la secuencia para obtener (batch_size, features)\n",
    "        output = output.mean(dim=1)\n",
    "        output = self.output_proj(output)\n",
    "        return output\n",
    "\n",
    "class RelayOptimizationPredictor:\n",
    "    def __init__(self, model_path, scaler_input_path, scaler_target_path, best_params_path):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Cargar par√°metros\n",
    "        with open(best_params_path, 'r') as f:\n",
    "            self.best_params = json.load(f)\n",
    "        \n",
    "        # Cargar scalers\n",
    "        with open(scaler_input_path, 'rb') as f:\n",
    "            self.scaler_input = pickle.load(f)\n",
    "        with open(scaler_target_path, 'rb') as f:\n",
    "            self.scaler_target = pickle.load(f)\n",
    "        \n",
    "        # Crear modelo\n",
    "        self.model = RelayOptimizationTransformer(\n",
    "            input_dim=6,\n",
    "            output_dim=4,\n",
    "            d_model=self.best_params['d_model'],\n",
    "            nhead=self.best_params['nhead'],\n",
    "            num_encoder_layers=self.best_params['num_encoder_layers'],\n",
    "            dim_feedforward=self.best_params['dim_feedforward'],\n",
    "            dropout=self.best_params['dropout']\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Cargar pesos\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict_optimization(self, relay_data):\n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for relay_pair in relay_data:\n",
    "                # Preparar caracter√≠sticas de entrada\n",
    "                input_features = [\n",
    "                    float(relay_pair['fault']),\n",
    "                    relay_pair['main_relay']['Ishc'],\n",
    "                    relay_pair['main_relay']['Time_out'],\n",
    "                    relay_pair['backup_relay']['Ishc'],\n",
    "                    relay_pair['backup_relay']['Time_out'],\n",
    "                    len(relay_data)\n",
    "                ]\n",
    "                \n",
    "                # Normalizar entrada\n",
    "                input_normalized = self.scaler_input.transform([input_features])\n",
    "                \n",
    "                # Convertir a tensor\n",
    "                input_tensor = torch.tensor(input_normalized, dtype=torch.float32).to(self.device)\n",
    "                \n",
    "                # Hacer predicci√≥n\n",
    "                prediction = self.model(input_tensor)\n",
    "                prediction_np = prediction.cpu().numpy().reshape(-1, 4)[0]\n",
    "                \n",
    "                # Desnormalizar predicci√≥n\n",
    "                prediction_denorm = self.scaler_target.inverse_transform([prediction_np])[0]\n",
    "                \n",
    "                # Crear resultado\n",
    "                result = {{\n",
    "                    'main_relay': {{\n",
    "                        'relay': relay_pair['main_relay']['relay'],\n",
    "                        'TDS': max(0.05, min(0.8, prediction_denorm[0])),\n",
    "                        'pickup': max(0.05, min(2.0, prediction_denorm[1]))\n",
    "                    }},\n",
    "                    'backup_relay': {{\n",
    "                        'relay': relay_pair['backup_relay']['relay'],\n",
    "                        'TDS': max(0.05, min(0.8, prediction_denorm[2])),\n",
    "                        'pickup': max(0.05, min(2.0, prediction_denorm[3]))\n",
    "                    }}\n",
    "                }}\n",
    "                \n",
    "                predictions.append(result)\n",
    "        \n",
    "        return predictions\n",
    "\"\"\"\n",
    "\n",
    "# Guardar archivo predictor\n",
    "predictor_path = MODEL_DIR / \"transformer_predictor.py\"\n",
    "with open(predictor_path, 'w') as f:\n",
    "    f.write(predictor_code)\n",
    "\n",
    "print(f\"‚úÖ Predictor class saved: {predictor_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXECUTION COMPLETED\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ TRANSFORMER TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä TRAINING SUMMARY:\")\n",
    "print(f\"   ‚Ä¢ Total training samples: {len(X_train)}\")\n",
    "print(f\"   ‚Ä¢ Total validation samples: {len(X_val)}\")\n",
    "print(f\"   ‚Ä¢ Best validation loss: {best_val_loss:.6f}\")\n",
    "print(f\"   ‚Ä¢ Final R¬≤ score: {r2:.6f}\")\n",
    "print(f\"   ‚Ä¢ Training epochs: {len(train_losses)}\")\n",
    "\n",
    "print(f\"\\nüìÅ FILES CREATED:\")\n",
    "print(f\"   ‚Ä¢ Model: {MODEL_PATH}\")\n",
    "print(f\"   ‚Ä¢ Input scaler: {SCALER_INPUT_PATH}\")\n",
    "print(f\"   ‚Ä¢ Target scaler: {SCALER_TARGET_PATH}\")\n",
    "print(f\"   ‚Ä¢ Best parameters: {BEST_PARAMS_PATH}\")\n",
    "print(f\"   ‚Ä¢ Training summary: {TRAINING_SUMMARY_PATH}\")\n",
    "print(f\"   ‚Ä¢ Predictor class: {predictor_path}\")\n",
    "\n",
    "print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "print(f\"   1. Run the validation notebook to test the model\")\n",
    "print(f\"   2. Use the predictor class for new relay optimizations\")\n",
    "print(f\"   3. Deploy the model for production use\")\n",
    "\n",
    "print(f\"\\n‚úÖ All operations completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
