{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Training for Relay Optimization\n",
    "\n",
    "This notebook trains a transformer network with Optuna to learn from relay optimization data and generalize optimal values.\n",
    "\n",
    "## Objectives:\n",
    "1. Load existing GA optimization data\n",
    "2. Train a transformer to predict optimal TDS and pickup values\n",
    "3. Use Optuna to optimize model hyperparameters\n",
    "4. Generalize optimization values for new scenarios\n",
    "\n",
    "**üöÄ SINGLE CELL EXECUTION - Run All Button Compatible**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 16:00:38,773] A new study created in memory with name: no-name-81c41f62-c8a7-4760-96ea-78397401abdb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ TRANSFORMER TRAINING - COMPLETE EXECUTION\n",
      "============================================================\n",
      "Using device: cpu\n",
      "üìÅ Model directory: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer\n",
      "üìÇ Data paths configured\n",
      "‚úÖ Transformer model architecture defined\n",
      "üîÑ Loading and processing data...\n",
      "üìä Raw data loaded: 6800 relay pairs\n",
      "üìä GA results loaded: 68 scenarios\n",
      "üìä Training dataset created: 6732 samples\n",
      "üìä Scenarios included: 68\n",
      "üîÑ Preparing training data...\n",
      "üìä Training samples: 5385\n",
      "üìä Validation samples: 1347\n",
      "üìä Input features: 6\n",
      "üìä Output features: 4\n",
      "‚úÖ Training functions defined\n",
      "üîÑ Starting Optuna optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 16:00:50,559] Trial 0 finished with value: 0.7245664488185536 and parameters: {'d_model': 32, 'nhead': 8, 'num_encoder_layers': 6, 'dim_feedforward': 256, 'dropout': 0.20197803879949555, 'learning_rate': 1.6838767700684057e-05, 'batch_size': 64, 'weight_decay': 1.3016553870000017e-06}. Best is trial 0 with value: 0.7245664488185536.\n",
      "[I 2025-10-10 16:00:59,803] Trial 1 finished with value: 0.7168317274613814 and parameters: {'d_model': 32, 'nhead': 4, 'num_encoder_layers': 5, 'dim_feedforward': 256, 'dropout': 0.11585560164459728, 'learning_rate': 1.7295984275108093e-05, 'batch_size': 64, 'weight_decay': 1.2872142405680262e-05}. Best is trial 1 with value: 0.7168317274613814.\n",
      "[I 2025-10-10 16:01:14,819] Trial 2 finished with value: 0.6357406227027669 and parameters: {'d_model': 32, 'nhead': 16, 'num_encoder_layers': 2, 'dim_feedforward': 512, 'dropout': 0.15975090887611088, 'learning_rate': 0.00011371779489995846, 'batch_size': 16, 'weight_decay': 4.283867350353657e-06}. Best is trial 2 with value: 0.6357406227027669.\n",
      "[I 2025-10-10 16:01:34,708] Trial 3 finished with value: 0.6350488827509039 and parameters: {'d_model': 32, 'nhead': 16, 'num_encoder_layers': 3, 'dim_feedforward': 256, 'dropout': 0.249745641107357, 'learning_rate': 0.00014151956377086362, 'batch_size': 16, 'weight_decay': 4.348768300011298e-06}. Best is trial 3 with value: 0.6350488827509039.\n",
      "[I 2025-10-10 16:02:04,602] Trial 4 finished with value: 0.6390790378346163 and parameters: {'d_model': 128, 'nhead': 16, 'num_encoder_layers': 4, 'dim_feedforward': 256, 'dropout': 0.17090117146794984, 'learning_rate': 8.980283747263797e-05, 'batch_size': 16, 'weight_decay': 1.8953068635720345e-06}. Best is trial 3 with value: 0.6350488827509039.\n",
      "[I 2025-10-10 16:02:17,216] Trial 5 finished with value: 0.6436463955570669 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 4, 'dim_feedforward': 256, 'dropout': 0.12327654970037688, 'learning_rate': 0.0007263773489580817, 'batch_size': 16, 'weight_decay': 8.347034170296804e-05}. Best is trial 3 with value: 0.6350488827509039.\n",
      "[I 2025-10-10 16:02:27,613] Trial 6 finished with value: 0.6410185729348382 and parameters: {'d_model': 128, 'nhead': 16, 'num_encoder_layers': 2, 'dim_feedforward': 1024, 'dropout': 0.22471132498973767, 'learning_rate': 4.599697930359468e-05, 'batch_size': 32, 'weight_decay': 1.2884993884057344e-06}. Best is trial 3 with value: 0.6350488827509039.\n",
      "[I 2025-10-10 16:02:52,448] Trial 7 finished with value: 0.6342369812376359 and parameters: {'d_model': 64, 'nhead': 16, 'num_encoder_layers': 3, 'dim_feedforward': 512, 'dropout': 0.17134816027072203, 'learning_rate': 0.0001965138530898775, 'batch_size': 16, 'weight_decay': 3.227104112678082e-06}. Best is trial 7 with value: 0.6342369812376359.\n",
      "[I 2025-10-10 16:02:56,584] Trial 8 pruned. \n",
      "[I 2025-10-10 16:03:05,051] Trial 9 finished with value: 0.645802360634471 and parameters: {'d_model': 128, 'nhead': 4, 'num_encoder_layers': 4, 'dim_feedforward': 256, 'dropout': 0.25498527796635556, 'learning_rate': 0.00046478636776606, 'batch_size': 32, 'weight_decay': 7.33771121134446e-05}. Best is trial 7 with value: 0.6342369812376359.\n",
      "[I 2025-10-10 16:03:26,618] Trial 10 pruned. \n",
      "[I 2025-10-10 16:03:59,481] Trial 11 finished with value: 0.6330097037203172 and parameters: {'d_model': 64, 'nhead': 16, 'num_encoder_layers': 3, 'dim_feedforward': 1024, 'dropout': 0.24406387170152652, 'learning_rate': 0.0002356533849024438, 'batch_size': 16, 'weight_decay': 3.5554966458120447e-06}. Best is trial 11 with value: 0.6330097037203172.\n",
      "[I 2025-10-10 16:04:18,964] Trial 12 finished with value: 0.6418191580211415 and parameters: {'d_model': 64, 'nhead': 16, 'num_encoder_layers': 3, 'dim_feedforward': 1024, 'dropout': 0.17242791112508318, 'learning_rate': 0.00030342160062827027, 'batch_size': 16, 'weight_decay': 2.297081311923656e-06}. Best is trial 11 with value: 0.6330097037203172.\n",
      "[I 2025-10-10 16:04:38,919] Trial 13 finished with value: 0.6390911382787368 and parameters: {'d_model': 64, 'nhead': 16, 'num_encoder_layers': 2, 'dim_feedforward': 1024, 'dropout': 0.29985290537904, 'learning_rate': 0.00022651923733957733, 'batch_size': 16, 'weight_decay': 8.589719586839891e-06}. Best is trial 11 with value: 0.6330097037203172.\n",
      "[I 2025-10-10 16:05:00,429] Trial 14 finished with value: 0.9145006432252771 and parameters: {'d_model': 64, 'nhead': 16, 'num_encoder_layers': 5, 'dim_feedforward': 1024, 'dropout': 0.24077555321880204, 'learning_rate': 0.0008890985704119542, 'batch_size': 16, 'weight_decay': 2.9845100445234464e-06}. Best is trial 11 with value: 0.6330097037203172.\n",
      "[I 2025-10-10 16:05:07,704] Trial 15 pruned. \n",
      "[I 2025-10-10 16:05:27,929] Trial 16 finished with value: 0.6551054228754605 and parameters: {'d_model': 64, 'nhead': 16, 'num_encoder_layers': 5, 'dim_feedforward': 512, 'dropout': 0.26856013697889264, 'learning_rate': 0.00042273198757641054, 'batch_size': 16, 'weight_decay': 1.0031542930304393e-06}. Best is trial 11 with value: 0.6330097037203172.\n",
      "[I 2025-10-10 16:05:50,558] Trial 17 finished with value: 0.6341664321282331 and parameters: {'d_model': 64, 'nhead': 16, 'num_encoder_layers': 2, 'dim_feedforward': 1024, 'dropout': 0.18911092108168667, 'learning_rate': 0.000167444483365706, 'batch_size': 16, 'weight_decay': 8.624654734538457e-06}. Best is trial 11 with value: 0.6330097037203172.\n",
      "[I 2025-10-10 16:05:54,770] Trial 18 pruned. \n",
      "[I 2025-10-10 16:06:02,374] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Optuna optimization completed!\n",
      "üìä Best trial: 11\n",
      "üìä Best validation loss: 0.633010\n",
      "üìä Best parameters: {'d_model': 64, 'nhead': 16, 'num_encoder_layers': 3, 'dim_feedforward': 1024, 'dropout': 0.24406387170152652, 'learning_rate': 0.0002356533849024438, 'batch_size': 16, 'weight_decay': 3.5554966458120447e-06}\n",
      "üîÑ Training final model with best parameters...\n",
      "üîÑ Training final model...\n",
      "   Epoch   0: Train Loss = 0.748726, Val Loss = 0.673977\n",
      "   Epoch   5: Train Loss = 0.656779, Val Loss = 0.643585\n",
      "   Epoch  10: Train Loss = 0.643616, Val Loss = 0.651486\n",
      "   Epoch  15: Train Loss = 0.635961, Val Loss = 0.638701\n",
      "   Epoch  20: Train Loss = 0.633849, Val Loss = 0.639975\n",
      "   Epoch  25: Train Loss = 0.634222, Val Loss = 0.644310\n",
      "   Epoch  30: Train Loss = 0.631504, Val Loss = 0.639542\n",
      "   Epoch  35: Train Loss = 0.629157, Val Loss = 0.638274\n",
      "   Epoch  40: Train Loss = 0.624997, Val Loss = 0.642147\n",
      "   Epoch  45: Train Loss = 0.622501, Val Loss = 0.637478\n",
      "   Early stopping at epoch 47\n",
      "‚úÖ Final model training completed!\n",
      "üìä Best validation loss: 0.633111\n",
      "üìä Total epochs: 48\n",
      "üîÑ Saving model and artifacts...\n",
      "‚úÖ Model and artifacts saved!\n",
      "üìÅ Model file: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/best_relay_optimization_transformer.pth\n",
      "üìÅ Scaler input: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/scaler_input.pkl\n",
      "üìÅ Scaler target: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/scaler_target.pkl\n",
      "üìÅ Best params: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/best_params.json\n",
      "üìÅ Training summary: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/training_summary.json\n",
      "üîÑ Evaluating final model...\n",
      "üìä Final Model Performance:\n",
      "   ‚Ä¢ MSE: 0.044356\n",
      "   ‚Ä¢ MAE: 0.141912\n",
      "   ‚Ä¢ R¬≤: 0.351238\n",
      "   ‚Ä¢ RMSE: 0.210609\n",
      "‚úÖ Predictor class saved: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/transformer_predictor.py\n",
      "\n",
      "============================================================\n",
      "üéâ TRANSFORMER TRAINING COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "\n",
      "üìä TRAINING SUMMARY:\n",
      "   ‚Ä¢ Total training samples: 5385\n",
      "   ‚Ä¢ Total validation samples: 1347\n",
      "   ‚Ä¢ Best validation loss: 0.633111\n",
      "   ‚Ä¢ Final R¬≤ score: 0.351238\n",
      "   ‚Ä¢ Training epochs: 48\n",
      "\n",
      "üìÅ FILES CREATED:\n",
      "   ‚Ä¢ Model: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/best_relay_optimization_transformer.pth\n",
      "   ‚Ä¢ Input scaler: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/scaler_input.pkl\n",
      "   ‚Ä¢ Target scaler: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/scaler_target.pkl\n",
      "   ‚Ä¢ Best parameters: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/best_params.json\n",
      "   ‚Ä¢ Training summary: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/training_summary.json\n",
      "   ‚Ä¢ Predictor class: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/transformer_predictor.py\n",
      "\n",
      "üöÄ NEXT STEPS:\n",
      "   1. Run the validation notebook to test the model\n",
      "   2. Use the predictor class for new relay optimizations\n",
      "   3. Deploy the model for production use\n",
      "\n",
      "‚úÖ All operations completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TRANSFORMER TRAINING FOR RELAY OPTIMIZATION - COMPLETE EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import math\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üöÄ TRANSFORMER TRAINING - COMPLETE EXECUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION AND PATHS\n",
    "# =============================================================================\n",
    "\n",
    "PROJECT_ROOT = Path(\"/Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG\")\n",
    "MODEL_DIR = PROJECT_ROOT / \"models\" / \"transformer\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RAW_DATA_PATH = PROJECT_ROOT / \"data\" / \"raw\" / \"automation_results.json\"\n",
    "OPTIMIZATION_RESULTS_PATH = PROJECT_ROOT / \"data\" / \"processed\" / \"ga_optimization_all_scenarios_comprehensive_20251008_224215.json\"\n",
    "\n",
    "# Model files\n",
    "MODEL_PATH = MODEL_DIR / \"best_relay_optimization_transformer.pth\"\n",
    "SCALER_INPUT_PATH = MODEL_DIR / \"scaler_input.pkl\"\n",
    "SCALER_TARGET_PATH = MODEL_DIR / \"scaler_target.pkl\"\n",
    "BEST_PARAMS_PATH = MODEL_DIR / \"best_params.json\"\n",
    "TRAINING_SUMMARY_PATH = MODEL_DIR / \"training_summary.json\"\n",
    "\n",
    "print(f\"üìÅ Model directory: {MODEL_DIR}\")\n",
    "print(f\"üìÇ Data paths configured\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRANSFORMER MODEL ARCHITECTURE\n",
    "# =============================================================================\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class RelayOptimizationTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(RelayOptimizationTransformer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.output_proj = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.input_proj.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output_proj.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Asegurar que src tenga 3 dimensiones (batch_size, sequence_length, features)\n",
    "        if src.dim() == 2:\n",
    "            # Si src tiene 2 dimensiones, agregar una dimensi√≥n de secuencia\n",
    "            src = src.unsqueeze(1)  # (batch_size, 1, features)\n",
    "        \n",
    "        src = self.input_proj(src) * math.sqrt(self.d_model)\n",
    "        # Transformer espera (sequence_length, batch_size, features)\n",
    "        src = src.permute(1, 0, 2)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        # Volver a (batch_size, sequence_length, features)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        # Promedio sobre la secuencia para obtener (batch_size, features)\n",
    "        output = output.mean(dim=1)\n",
    "        output = self.output_proj(output)\n",
    "        return output\n",
    "\n",
    "print(\"‚úÖ Transformer model architecture defined\")\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_process_data():\n",
    "    \"\"\"\n",
    "    Carga y procesa los datos de rel√©s y optimizaci√≥n GA\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Loading and processing data...\")\n",
    "    \n",
    "    # Cargar datos originales\n",
    "    with open(RAW_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    # Cargar resultados de optimizaci√≥n GA\n",
    "    with open(OPTIMIZATION_RESULTS_PATH, 'r', encoding='utf-8') as f:\n",
    "        ga_results = json.load(f)\n",
    "    \n",
    "    print(f\"üìä Raw data loaded: {len(raw_data)} relay pairs\")\n",
    "    print(f\"üìä GA results loaded: {len(ga_results['optimization_results'])} scenarios\")\n",
    "    \n",
    "    # Agrupar datos originales por escenario\n",
    "    raw_by_scenario = defaultdict(list)\n",
    "    for entry in raw_data:\n",
    "        raw_by_scenario[entry['scenario_id']].append(entry)\n",
    "    \n",
    "    # Obtener resultados GA por escenario\n",
    "    ga_by_scenario = ga_results['optimization_results']\n",
    "    \n",
    "    # Crear dataset de entrenamiento\n",
    "    training_data = []\n",
    "    \n",
    "    for scenario_id in ga_by_scenario.keys():\n",
    "        if scenario_id not in raw_by_scenario:\n",
    "            continue\n",
    "            \n",
    "        scenario_raw = raw_by_scenario[scenario_id]\n",
    "        scenario_ga = ga_by_scenario[scenario_id]\n",
    "        \n",
    "        optimized_relays = scenario_ga['relay_values']\n",
    "        \n",
    "        for relay_pair in scenario_raw:\n",
    "            main_relay_id = relay_pair['main_relay']['relay']\n",
    "            backup_relay_id = relay_pair['backup_relay']['relay']\n",
    "            \n",
    "            # Verificar si ambos rel√©s fueron optimizados\n",
    "            if main_relay_id in optimized_relays and backup_relay_id in optimized_relays:\n",
    "                \n",
    "                # Caracter√≠sticas de entrada\n",
    "                input_features = [\n",
    "                    float(relay_pair['fault']),\n",
    "                    relay_pair['main_relay']['Ishc'],\n",
    "                    relay_pair['main_relay']['Time_out'],\n",
    "                    relay_pair['backup_relay']['Ishc'],\n",
    "                    relay_pair['backup_relay']['Time_out'],\n",
    "                    len(scenario_raw)\n",
    "                ]\n",
    "                \n",
    "                # Caracter√≠sticas objetivo (valores optimizados por GA)\n",
    "                target_features = [\n",
    "                    optimized_relays[main_relay_id]['TDS'],\n",
    "                    optimized_relays[main_relay_id]['pickup'],\n",
    "                    optimized_relays[backup_relay_id]['TDS'],\n",
    "                    optimized_relays[backup_relay_id]['pickup']\n",
    "                ]\n",
    "                \n",
    "                training_data.append({\n",
    "                    'input': input_features,\n",
    "                    'target': target_features,\n",
    "                    'scenario_id': scenario_id,\n",
    "                    'main_relay': main_relay_id,\n",
    "                    'backup_relay': backup_relay_id\n",
    "                })\n",
    "    \n",
    "    print(f\"üìä Training dataset created: {len(training_data)} samples\")\n",
    "    print(f\"üìä Scenarios included: {len(set(d['scenario_id'] for d in training_data))}\")\n",
    "    \n",
    "    if len(training_data) == 0:\n",
    "        raise ValueError(\"No training data created. Check data files.\")\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "# Cargar datos\n",
    "training_data = load_and_process_data()\n",
    "\n",
    "# =============================================================================\n",
    "# DATA PREPARATION FOR TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_training_data(training_data):\n",
    "    \"\"\"\n",
    "    Prepara los datos para el entrenamiento\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Preparing training data...\")\n",
    "    \n",
    "    # Extraer caracter√≠sticas de entrada y objetivo\n",
    "    X = np.array([item['input'] for item in training_data])\n",
    "    y = np.array([item['target'] for item in training_data])\n",
    "    \n",
    "    # Dividir en entrenamiento y validaci√≥n\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Normalizar caracter√≠sticas de entrada\n",
    "    scaler_input = StandardScaler()\n",
    "    X_train_scaled = scaler_input.fit_transform(X_train)\n",
    "    X_val_scaled = scaler_input.transform(X_val)\n",
    "    \n",
    "    # Normalizar caracter√≠sticas objetivo\n",
    "    scaler_target = StandardScaler()\n",
    "    y_train_scaled = scaler_target.fit_transform(y_train)\n",
    "    y_val_scaled = scaler_target.transform(y_val)\n",
    "    \n",
    "    print(f\"üìä Training samples: {len(X_train_scaled)}\")\n",
    "    print(f\"üìä Validation samples: {len(X_val_scaled)}\")\n",
    "    print(f\"üìä Input features: {X_train_scaled.shape[1]}\")\n",
    "    print(f\"üìä Output features: {y_train_scaled.shape[1]}\")\n",
    "    \n",
    "    return (X_train_scaled, X_val_scaled, y_train_scaled, y_val_scaled, \n",
    "            scaler_input, scaler_target)\n",
    "\n",
    "# Preparar datos\n",
    "X_train, X_val, y_train, y_val, scaler_input, scaler_target = prepare_training_data(training_data)\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Entrena el modelo por una √©poca\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in dataloader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Valida el modelo por una √©poca\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in dataloader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Funci√≥n objetivo para Optuna\n",
    "    \"\"\"\n",
    "    # Par√°metros del modelo\n",
    "    d_model = trial.suggest_categorical('d_model', [32, 64, 128])\n",
    "    nhead = trial.suggest_categorical('nhead', [4, 8, 16])\n",
    "    num_encoder_layers = trial.suggest_int('num_encoder_layers', 2, 6)\n",
    "    dim_feedforward = trial.suggest_categorical('dim_feedforward', [256, 512, 1024])\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.3)\n",
    "    \n",
    "    # Par√°metros de entrenamiento\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-4, log=True)\n",
    "    \n",
    "    # Crear modelo\n",
    "    model = RelayOptimizationTransformer(\n",
    "        input_dim=6,\n",
    "        output_dim=4,\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizador y funci√≥n de p√©rdida\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Crear dataloaders\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.float32)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(20):  # M√°ximo 20 √©pocas por trial\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "        \n",
    "        # Reportar m√©tricas intermedias\n",
    "        trial.report(val_loss, epoch)\n",
    "        \n",
    "        # Verificar si el trial debe ser podado\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return best_val_loss\n",
    "\n",
    "print(\"‚úÖ Training functions defined\")\n",
    "\n",
    "# =============================================================================\n",
    "# OPTUNA OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîÑ Starting Optuna optimization...\")\n",
    "\n",
    "# Crear estudio Optuna\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    ")\n",
    "\n",
    "# Ejecutar optimizaci√≥n\n",
    "study.optimize(objective, n_trials=20, timeout=1800)  # 20 trials, 30 min timeout\n",
    "\n",
    "print(f\"‚úÖ Optuna optimization completed!\")\n",
    "print(f\"üìä Best trial: {study.best_trial.number}\")\n",
    "print(f\"üìä Best validation loss: {study.best_value:.6f}\")\n",
    "print(f\"üìä Best parameters: {study.best_params}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL MODEL TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîÑ Training final model with best parameters...\")\n",
    "\n",
    "# Obtener mejores par√°metros\n",
    "best_params = study.best_params\n",
    "\n",
    "# Crear modelo final\n",
    "final_model = RelayOptimizationTransformer(\n",
    "    input_dim=6,\n",
    "    output_dim=4,\n",
    "    d_model=best_params['d_model'],\n",
    "    nhead=best_params['nhead'],\n",
    "    num_encoder_layers=best_params['num_encoder_layers'],\n",
    "    dim_feedforward=best_params['dim_feedforward'],\n",
    "    dropout=best_params['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Optimizador y funci√≥n de p√©rdida\n",
    "optimizer = optim.Adam(\n",
    "    final_model.parameters(), \n",
    "    lr=best_params['learning_rate'], \n",
    "    weight_decay=best_params['weight_decay']\n",
    ")\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Crear dataloaders\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.float32)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.tensor(X_val, dtype=torch.float32),\n",
    "    torch.tensor(y_val, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "# Entrenar modelo final\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"üîÑ Training final model...\")\n",
    "\n",
    "for epoch in range(50):  # M√°ximo 50 √©pocas\n",
    "    train_loss = train_epoch(final_model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = validate_epoch(final_model, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"   Epoch {epoch:3d}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Guardar mejor modelo\n",
    "        torch.save(final_model.state_dict(), MODEL_PATH)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"   Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "print(f\"‚úÖ Final model training completed!\")\n",
    "print(f\"üìä Best validation loss: {best_val_loss:.6f}\")\n",
    "print(f\"üìä Total epochs: {len(train_losses)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE MODEL AND ARTIFACTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîÑ Saving model and artifacts...\")\n",
    "\n",
    "# Guardar scalers\n",
    "with open(SCALER_INPUT_PATH, 'wb') as f:\n",
    "    pickle.dump(scaler_input, f)\n",
    "\n",
    "with open(SCALER_TARGET_PATH, 'wb') as f:\n",
    "    pickle.dump(scaler_target, f)\n",
    "\n",
    "# Guardar mejores par√°metros\n",
    "with open(BEST_PARAMS_PATH, 'w') as f:\n",
    "    json.dump(best_params, f, indent=4)\n",
    "\n",
    "# Guardar resumen de entrenamiento\n",
    "training_summary = {\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'best_params': best_params,\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'num_final_epochs': len(train_losses),\n",
    "    'training_date': time.strftime('%Y-%m-%dT%H:%M:%S.%f'),\n",
    "    'mode': 'SINGLE_CELL_EXECUTION'\n",
    "}\n",
    "\n",
    "with open(TRAINING_SUMMARY_PATH, 'w') as f:\n",
    "    json.dump(training_summary, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Model and artifacts saved!\")\n",
    "print(f\"üìÅ Model file: {MODEL_PATH}\")\n",
    "print(f\"üìÅ Scaler input: {SCALER_INPUT_PATH}\")\n",
    "print(f\"üìÅ Scaler target: {SCALER_TARGET_PATH}\")\n",
    "print(f\"üìÅ Best params: {BEST_PARAMS_PATH}\")\n",
    "print(f\"üìÅ Training summary: {TRAINING_SUMMARY_PATH}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîÑ Evaluating final model...\")\n",
    "\n",
    "# Cargar mejor modelo\n",
    "final_model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "final_model.eval()\n",
    "\n",
    "# Evaluar en datos de validaci√≥n\n",
    "with torch.no_grad():\n",
    "    val_predictions = final_model(torch.tensor(X_val, dtype=torch.float32).to(device))\n",
    "    val_predictions = val_predictions.cpu().numpy()\n",
    "    \n",
    "    # Desnormalizar predicciones\n",
    "    val_predictions_denorm = scaler_target.inverse_transform(val_predictions)\n",
    "    val_targets_denorm = scaler_target.inverse_transform(y_val)\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    \n",
    "    mse = mean_squared_error(val_targets_denorm, val_predictions_denorm)\n",
    "    mae = mean_absolute_error(val_targets_denorm, val_predictions_denorm)\n",
    "    r2 = r2_score(val_targets_denorm, val_predictions_denorm)\n",
    "    \n",
    "    print(f\"üìä Final Model Performance:\")\n",
    "    print(f\"   ‚Ä¢ MSE: {mse:.6f}\")\n",
    "    print(f\"   ‚Ä¢ MAE: {mae:.6f}\")\n",
    "    print(f\"   ‚Ä¢ R¬≤: {r2:.6f}\")\n",
    "    print(f\"   ‚Ä¢ RMSE: {np.sqrt(mse):.6f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRANSFORMER PREDICTOR CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class RelayOptimizationPredictor:\n",
    "    \"\"\"\n",
    "    Clase para hacer predicciones con el modelo entrenado\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, scaler_input_path, scaler_target_path, best_params_path):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Cargar par√°metros\n",
    "        with open(best_params_path, 'r') as f:\n",
    "            self.best_params = json.load(f)\n",
    "        \n",
    "        # Cargar scalers\n",
    "        with open(scaler_input_path, 'rb') as f:\n",
    "            self.scaler_input = pickle.load(f)\n",
    "        with open(scaler_target_path, 'rb') as f:\n",
    "            self.scaler_target = pickle.load(f)\n",
    "        \n",
    "        # Crear modelo\n",
    "        self.model = RelayOptimizationTransformer(\n",
    "            input_dim=6,\n",
    "            output_dim=4,\n",
    "            d_model=self.best_params['d_model'],\n",
    "            nhead=self.best_params['nhead'],\n",
    "            num_encoder_layers=self.best_params['num_encoder_layers'],\n",
    "            dim_feedforward=self.best_params['dim_feedforward'],\n",
    "            dropout=self.best_params['dropout']\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Cargar pesos\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict_optimization(self, relay_data):\n",
    "        \"\"\"\n",
    "        Predice valores √≥ptimos para un par de rel√©s\n",
    "        \n",
    "        Args:\n",
    "            relay_data: Lista de diccionarios con datos de rel√©s\n",
    "        \n",
    "        Returns:\n",
    "            Lista de predicciones con valores √≥ptimos\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for relay_pair in relay_data:\n",
    "                # Preparar caracter√≠sticas de entrada\n",
    "                input_features = [\n",
    "                    float(relay_pair['fault']),\n",
    "                    relay_pair['main_relay']['Ishc'],\n",
    "                    relay_pair['main_relay']['Time_out'],\n",
    "                    relay_pair['backup_relay']['Ishc'],\n",
    "                    relay_pair['backup_relay']['Time_out'],\n",
    "                    len(relay_data)\n",
    "                ]\n",
    "                \n",
    "                # Normalizar entrada\n",
    "                input_normalized = self.scaler_input.transform([input_features])\n",
    "                \n",
    "                # Convertir a tensor\n",
    "                input_tensor = torch.tensor(input_normalized, dtype=torch.float32).to(self.device)\n",
    "                \n",
    "                # Hacer predicci√≥n\n",
    "                prediction = self.model(input_tensor)\n",
    "                prediction_np = prediction.cpu().numpy().reshape(-1, 4)[0]\n",
    "                \n",
    "                # Desnormalizar predicci√≥n\n",
    "                prediction_denorm = self.scaler_target.inverse_transform([prediction_np])[0]\n",
    "                \n",
    "                # Crear resultado\n",
    "                result = {\n",
    "                    'main_relay': {\n",
    "                        'relay': relay_pair['main_relay']['relay'],\n",
    "                        'TDS': max(0.05, min(0.8, prediction_denorm[0])),\n",
    "                        'pickup': max(0.05, min(2.0, prediction_denorm[1]))\n",
    "                    },\n",
    "                    'backup_relay': {\n",
    "                        'relay': relay_pair['backup_relay']['relay'],\n",
    "                        'TDS': max(0.05, min(0.8, prediction_denorm[2])),\n",
    "                        'pickup': max(0.05, min(2.0, prediction_denorm[3]))\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                predictions.append(result)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Guardar clase predictor\n",
    "predictor_code = f\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class RelayOptimizationTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(RelayOptimizationTransformer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.output_proj = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.input_proj.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output_proj.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Asegurar que src tenga 3 dimensiones (batch_size, sequence_length, features)\n",
    "        if src.dim() == 2:\n",
    "            # Si src tiene 2 dimensiones, agregar una dimensi√≥n de secuencia\n",
    "            src = src.unsqueeze(1)  # (batch_size, 1, features)\n",
    "        \n",
    "        src = self.input_proj(src) * math.sqrt(self.d_model)\n",
    "        # Transformer espera (sequence_length, batch_size, features)\n",
    "        src = src.permute(1, 0, 2)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        # Volver a (batch_size, sequence_length, features)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        # Promedio sobre la secuencia para obtener (batch_size, features)\n",
    "        output = output.mean(dim=1)\n",
    "        output = self.output_proj(output)\n",
    "        return output\n",
    "\n",
    "class RelayOptimizationPredictor:\n",
    "    def __init__(self, model_path, scaler_input_path, scaler_target_path, best_params_path):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Cargar par√°metros\n",
    "        with open(best_params_path, 'r') as f:\n",
    "            self.best_params = json.load(f)\n",
    "        \n",
    "        # Cargar scalers\n",
    "        with open(scaler_input_path, 'rb') as f:\n",
    "            self.scaler_input = pickle.load(f)\n",
    "        with open(scaler_target_path, 'rb') as f:\n",
    "            self.scaler_target = pickle.load(f)\n",
    "        \n",
    "        # Crear modelo\n",
    "        self.model = RelayOptimizationTransformer(\n",
    "            input_dim=6,\n",
    "            output_dim=4,\n",
    "            d_model=self.best_params['d_model'],\n",
    "            nhead=self.best_params['nhead'],\n",
    "            num_encoder_layers=self.best_params['num_encoder_layers'],\n",
    "            dim_feedforward=self.best_params['dim_feedforward'],\n",
    "            dropout=self.best_params['dropout']\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Cargar pesos\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict_optimization(self, relay_data):\n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for relay_pair in relay_data:\n",
    "                # Preparar caracter√≠sticas de entrada\n",
    "                input_features = [\n",
    "                    float(relay_pair['fault']),\n",
    "                    relay_pair['main_relay']['Ishc'],\n",
    "                    relay_pair['main_relay']['Time_out'],\n",
    "                    relay_pair['backup_relay']['Ishc'],\n",
    "                    relay_pair['backup_relay']['Time_out'],\n",
    "                    len(relay_data)\n",
    "                ]\n",
    "                \n",
    "                # Normalizar entrada\n",
    "                input_normalized = self.scaler_input.transform([input_features])\n",
    "                \n",
    "                # Convertir a tensor\n",
    "                input_tensor = torch.tensor(input_normalized, dtype=torch.float32).to(self.device)\n",
    "                \n",
    "                # Hacer predicci√≥n\n",
    "                prediction = self.model(input_tensor)\n",
    "                prediction_np = prediction.cpu().numpy().reshape(-1, 4)[0]\n",
    "                \n",
    "                # Desnormalizar predicci√≥n\n",
    "                prediction_denorm = self.scaler_target.inverse_transform([prediction_np])[0]\n",
    "                \n",
    "                # Crear resultado\n",
    "                result = {{\n",
    "                    'main_relay': {{\n",
    "                        'relay': relay_pair['main_relay']['relay'],\n",
    "                        'TDS': max(0.05, min(0.8, prediction_denorm[0])),\n",
    "                        'pickup': max(0.05, min(2.0, prediction_denorm[1]))\n",
    "                    }},\n",
    "                    'backup_relay': {{\n",
    "                        'relay': relay_pair['backup_relay']['relay'],\n",
    "                        'TDS': max(0.05, min(0.8, prediction_denorm[2])),\n",
    "                        'pickup': max(0.05, min(2.0, prediction_denorm[3]))\n",
    "                    }}\n",
    "                }}\n",
    "                \n",
    "                predictions.append(result)\n",
    "        \n",
    "        return predictions\n",
    "\"\"\"\n",
    "\n",
    "# Guardar archivo predictor\n",
    "predictor_path = MODEL_DIR / \"transformer_predictor.py\"\n",
    "with open(predictor_path, 'w') as f:\n",
    "    f.write(predictor_code)\n",
    "\n",
    "print(f\"‚úÖ Predictor class saved: {predictor_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXECUTION COMPLETED\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ TRANSFORMER TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä TRAINING SUMMARY:\")\n",
    "print(f\"   ‚Ä¢ Total training samples: {len(X_train)}\")\n",
    "print(f\"   ‚Ä¢ Total validation samples: {len(X_val)}\")\n",
    "print(f\"   ‚Ä¢ Best validation loss: {best_val_loss:.6f}\")\n",
    "print(f\"   ‚Ä¢ Final R¬≤ score: {r2:.6f}\")\n",
    "print(f\"   ‚Ä¢ Training epochs: {len(train_losses)}\")\n",
    "\n",
    "print(f\"\\nüìÅ FILES CREATED:\")\n",
    "print(f\"   ‚Ä¢ Model: {MODEL_PATH}\")\n",
    "print(f\"   ‚Ä¢ Input scaler: {SCALER_INPUT_PATH}\")\n",
    "print(f\"   ‚Ä¢ Target scaler: {SCALER_TARGET_PATH}\")\n",
    "print(f\"   ‚Ä¢ Best parameters: {BEST_PARAMS_PATH}\")\n",
    "print(f\"   ‚Ä¢ Training summary: {TRAINING_SUMMARY_PATH}\")\n",
    "print(f\"   ‚Ä¢ Predictor class: {predictor_path}\")\n",
    "\n",
    "print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "print(f\"   1. Run the validation notebook to test the model\")\n",
    "print(f\"   2. Use the predictor class for new relay optimizations\")\n",
    "print(f\"   3. Deploy the model for production use\")\n",
    "\n",
    "print(f\"\\n‚úÖ All operations completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
