{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Training for Relay Optimization\n",
    "\n",
    "This notebook trains a transformer network with Optuna to learn from relay optimization data and generalize optimal values.\n",
    "\n",
    "## Objectives:\n",
    "1. Load existing GA optimization data\n",
    "2. Train a transformer to predict optimal TDS and pickup values\n",
    "3. Use Optuna to optimize model hyperparameters\n",
    "4. Generalize optimization values for new scenarios\n",
    "\n",
    "**🚀 SINGLE CELL EXECUTION - Run All Button Compatible**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 16:00:38,773] A new study created in memory with name: no-name-81c41f62-c8a7-4760-96ea-78397401abdb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 TRANSFORMER TRAINING - COMPLETE EXECUTION\n",
      "============================================================\n",
      "Using device: cpu\n",
      "📁 Model directory: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer\n",
      "📂 Data paths configured\n",
      "✅ Transformer model architecture defined\n",
      "🔄 Loading and processing data...\n",
      "📊 Raw data loaded: 6800 relay pairs\n",
      "📊 GA results loaded: 68 scenarios\n",
      "📊 Training dataset created: 6732 samples\n",
      "📊 Scenarios included: 68\n",
      "🔄 Preparing training data...\n",
      "📊 Training samples: 5385\n",
      "📊 Validation samples: 1347\n",
      "📊 Input features: 6\n",
      "📊 Output features: 4\n",
      "✅ Training functions defined\n",
      "🔄 Starting Optuna optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 16:00:50,559] Trial 0 finished with value: 0.7245664488185536 and parameters: {'d_model': 32, 'nhead': 8, 'num_encoder_layers': 6, 'dim_feedforward': 256, 'dropout': 0.20197803879949555, 'learning_rate': 1.6838767700684057e-05, 'batch_size': 64, 'weight_decay': 1.3016553870000017e-06}. Best is trial 0 with value: 0.7245664488185536.\n",
      "[I 2025-10-10 16:00:59,803] Trial 1 finished with value: 0.7168317274613814 and parameters: {'d_model': 32, 'nhead': 4, 'num_encoder_layers': 5, 'dim_feedforward': 256, 'dropout': 0.11585560164459728, 'learning_rate': 1.7295984275108093e-05, 'batch_size': 64, 'weight_decay': 1.2872142405680262e-05}. Best is trial 1 with value: 0.7168317274613814.\n",
      "[I 2025-10-10 16:01:14,819] Trial 2 finished with value: 0.6357406227027669 and parameters: {'d_model': 32, 'nhead': 16, 'num_encoder_layers': 2, 'dim_feedforward': 512, 'dropout': 0.15975090887611088, 'learning_rate': 0.00011371779489995846, 'batch_size': 16, 'weight_decay': 4.283867350353657e-06}. Best is trial 2 with value: 0.6357406227027669.\n",
      "[I 2025-10-10 16:01:34,708] Trial 3 finished with value: 0.6350488827509039 and parameters: {'d_model': 32, 'nhead': 16, 'num_encoder_layers': 3, 'dim_feedforward': 256, 'dropout': 0.249745641107357, 'learning_rate': 0.00014151956377086362, 'batch_size': 16, 'weight_decay': 4.348768300011298e-06}. Best is trial 3 with value: 0.6350488827509039.\n",
      "[I 2025-10-10 16:02:04,602] Trial 4 finished with value: 0.6390790378346163 and parameters: {'d_model': 128, 'nhead': 16, 'num_encoder_layers': 4, 'dim_feedforward': 256, 'dropout': 0.17090117146794984, 'learning_rate': 8.980283747263797e-05, 'batch_size': 16, 'weight_decay': 1.8953068635720345e-06}. Best is trial 3 with value: 0.6350488827509039.\n",
      "[I 2025-10-10 16:02:17,216] Trial 5 finished with value: 0.6436463955570669 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 4, 'dim_feedforward': 256, 'dropout': 0.12327654970037688, 'learning_rate': 0.0007263773489580817, 'batch_size': 16, 'weight_decay': 8.347034170296804e-05}. Best is trial 3 with value: 0.6350488827509039.\n",
      "[I 2025-10-10 16:02:27,613] Trial 6 finished with value: 0.6410185729348382 and parameters: {'d_model': 128, 'nhead': 16, 'num_encoder_layers': 2, 'dim_feedforward': 1024, 'dropout': 0.22471132498973767, 'learning_rate': 4.599697930359468e-05, 'batch_size': 32, 'weight_decay': 1.2884993884057344e-06}. Best is trial 3 with value: 0.6350488827509039.\n",
      "[I 2025-10-10 16:02:52,448] Trial 7 finished with value: 0.6342369812376359 and parameters: {'d_model': 64, 'nhead': 16, 'num_encoder_layers': 3, 'dim_feedforward': 512, 'dropout': 0.17134816027072203, 'learning_rate': 0.0001965138530898775, 'batch_size': 16, 'weight_decay': 3.227104112678082e-06}. Best is trial 7 with value: 0.6342369812376359.\n",
      "[I 2025-10-10 16:02:56,584] Trial 8 pruned. \n",
      "[I 2025-10-10 16:03:05,051] Trial 9 finished with value: 0.645802360634471 and parameters: {'d_model': 128, 'nhead': 4, 'num_encoder_layers': 4, 'dim_feedforward': 256, 'dropout': 0.25498527796635556, 'learning_rate': 0.00046478636776606, 'batch_size': 32, 'weight_decay': 7.33771121134446e-05}. Best is trial 7 with value: 0.6342369812376359.\n",
      "[I 2025-10-10 16:03:26,618] Trial 10 pruned. \n",
      "[I 2025-10-10 16:03:59,481] Trial 11 finished with value: 0.6330097037203172 and parameters: {'d_model': 64, 'nhead': 16, 'num_encoder_layers': 3, 'dim_feedforward': 1024, 'dropout': 0.24406387170152652, 'learning_rate': 0.0002356533849024438, 'batch_size': 16, 'weight_decay': 3.5554966458120447e-06}. Best is trial 11 with value: 0.6330097037203172.\n",
      "[I 2025-10-10 16:04:18,964] Trial 12 finished with value: 0.6418191580211415 and parameters: {'d_model': 64, 'nhead': 16, 'num_encoder_layers': 3, 'dim_feedforward': 1024, 'dropout': 0.17242791112508318, 'learning_rate': 0.00030342160062827027, 'batch_size': 16, 'weight_decay': 2.297081311923656e-06}. Best is trial 11 with value: 0.6330097037203172.\n",
      "[I 2025-10-10 16:04:38,919] Trial 13 finished with value: 0.6390911382787368 and parameters: {'d_model': 64, 'nhead': 16, 'num_encoder_layers': 2, 'dim_feedforward': 1024, 'dropout': 0.29985290537904, 'learning_rate': 0.00022651923733957733, 'batch_size': 16, 'weight_decay': 8.589719586839891e-06}. Best is trial 11 with value: 0.6330097037203172.\n",
      "[I 2025-10-10 16:05:00,429] Trial 14 finished with value: 0.9145006432252771 and parameters: {'d_model': 64, 'nhead': 16, 'num_encoder_layers': 5, 'dim_feedforward': 1024, 'dropout': 0.24077555321880204, 'learning_rate': 0.0008890985704119542, 'batch_size': 16, 'weight_decay': 2.9845100445234464e-06}. Best is trial 11 with value: 0.6330097037203172.\n",
      "[I 2025-10-10 16:05:07,704] Trial 15 pruned. \n",
      "[I 2025-10-10 16:05:27,929] Trial 16 finished with value: 0.6551054228754605 and parameters: {'d_model': 64, 'nhead': 16, 'num_encoder_layers': 5, 'dim_feedforward': 512, 'dropout': 0.26856013697889264, 'learning_rate': 0.00042273198757641054, 'batch_size': 16, 'weight_decay': 1.0031542930304393e-06}. Best is trial 11 with value: 0.6330097037203172.\n",
      "[I 2025-10-10 16:05:50,558] Trial 17 finished with value: 0.6341664321282331 and parameters: {'d_model': 64, 'nhead': 16, 'num_encoder_layers': 2, 'dim_feedforward': 1024, 'dropout': 0.18911092108168667, 'learning_rate': 0.000167444483365706, 'batch_size': 16, 'weight_decay': 8.624654734538457e-06}. Best is trial 11 with value: 0.6330097037203172.\n",
      "[I 2025-10-10 16:05:54,770] Trial 18 pruned. \n",
      "[I 2025-10-10 16:06:02,374] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Optuna optimization completed!\n",
      "📊 Best trial: 11\n",
      "📊 Best validation loss: 0.633010\n",
      "📊 Best parameters: {'d_model': 64, 'nhead': 16, 'num_encoder_layers': 3, 'dim_feedforward': 1024, 'dropout': 0.24406387170152652, 'learning_rate': 0.0002356533849024438, 'batch_size': 16, 'weight_decay': 3.5554966458120447e-06}\n",
      "🔄 Training final model with best parameters...\n",
      "🔄 Training final model...\n",
      "   Epoch   0: Train Loss = 0.748726, Val Loss = 0.673977\n",
      "   Epoch   5: Train Loss = 0.656779, Val Loss = 0.643585\n",
      "   Epoch  10: Train Loss = 0.643616, Val Loss = 0.651486\n",
      "   Epoch  15: Train Loss = 0.635961, Val Loss = 0.638701\n",
      "   Epoch  20: Train Loss = 0.633849, Val Loss = 0.639975\n",
      "   Epoch  25: Train Loss = 0.634222, Val Loss = 0.644310\n",
      "   Epoch  30: Train Loss = 0.631504, Val Loss = 0.639542\n",
      "   Epoch  35: Train Loss = 0.629157, Val Loss = 0.638274\n",
      "   Epoch  40: Train Loss = 0.624997, Val Loss = 0.642147\n",
      "   Epoch  45: Train Loss = 0.622501, Val Loss = 0.637478\n",
      "   Early stopping at epoch 47\n",
      "✅ Final model training completed!\n",
      "📊 Best validation loss: 0.633111\n",
      "📊 Total epochs: 48\n",
      "🔄 Saving model and artifacts...\n",
      "✅ Model and artifacts saved!\n",
      "📁 Model file: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/best_relay_optimization_transformer.pth\n",
      "📁 Scaler input: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/scaler_input.pkl\n",
      "📁 Scaler target: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/scaler_target.pkl\n",
      "📁 Best params: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/best_params.json\n",
      "📁 Training summary: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/training_summary.json\n",
      "🔄 Evaluating final model...\n",
      "📊 Final Model Performance:\n",
      "   • MSE: 0.044356\n",
      "   • MAE: 0.141912\n",
      "   • R²: 0.351238\n",
      "   • RMSE: 0.210609\n",
      "✅ Predictor class saved: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/transformer_predictor.py\n",
      "\n",
      "============================================================\n",
      "🎉 TRANSFORMER TRAINING COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "\n",
      "📊 TRAINING SUMMARY:\n",
      "   • Total training samples: 5385\n",
      "   • Total validation samples: 1347\n",
      "   • Best validation loss: 0.633111\n",
      "   • Final R² score: 0.351238\n",
      "   • Training epochs: 48\n",
      "\n",
      "📁 FILES CREATED:\n",
      "   • Model: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/best_relay_optimization_transformer.pth\n",
      "   • Input scaler: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/scaler_input.pkl\n",
      "   • Target scaler: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/scaler_target.pkl\n",
      "   • Best parameters: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/best_params.json\n",
      "   • Training summary: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/training_summary.json\n",
      "   • Predictor class: /Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG/models/transformer/transformer_predictor.py\n",
      "\n",
      "🚀 NEXT STEPS:\n",
      "   1. Run the validation notebook to test the model\n",
      "   2. Use the predictor class for new relay optimizations\n",
      "   3. Deploy the model for production use\n",
      "\n",
      "✅ All operations completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TRANSFORMER TRAINING FOR RELAY OPTIMIZATION - COMPLETE EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import math\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"🚀 TRANSFORMER TRAINING - COMPLETE EXECUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION AND PATHS\n",
    "# =============================================================================\n",
    "\n",
    "PROJECT_ROOT = Path(\"/Users/gustavo/Documents/Projects/TESIS_UNAL/AutoDOC-MG\")\n",
    "MODEL_DIR = PROJECT_ROOT / \"models\" / \"transformer\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RAW_DATA_PATH = PROJECT_ROOT / \"data\" / \"raw\" / \"automation_results.json\"\n",
    "OPTIMIZATION_RESULTS_PATH = PROJECT_ROOT / \"data\" / \"processed\" / \"ga_optimization_all_scenarios_comprehensive_20251008_224215.json\"\n",
    "\n",
    "# Model files\n",
    "MODEL_PATH = MODEL_DIR / \"best_relay_optimization_transformer.pth\"\n",
    "SCALER_INPUT_PATH = MODEL_DIR / \"scaler_input.pkl\"\n",
    "SCALER_TARGET_PATH = MODEL_DIR / \"scaler_target.pkl\"\n",
    "BEST_PARAMS_PATH = MODEL_DIR / \"best_params.json\"\n",
    "TRAINING_SUMMARY_PATH = MODEL_DIR / \"training_summary.json\"\n",
    "\n",
    "print(f\"📁 Model directory: {MODEL_DIR}\")\n",
    "print(f\"📂 Data paths configured\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRANSFORMER MODEL ARCHITECTURE\n",
    "# =============================================================================\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class RelayOptimizationTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(RelayOptimizationTransformer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.output_proj = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.input_proj.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output_proj.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Asegurar que src tenga 3 dimensiones (batch_size, sequence_length, features)\n",
    "        if src.dim() == 2:\n",
    "            # Si src tiene 2 dimensiones, agregar una dimensión de secuencia\n",
    "            src = src.unsqueeze(1)  # (batch_size, 1, features)\n",
    "        \n",
    "        src = self.input_proj(src) * math.sqrt(self.d_model)\n",
    "        # Transformer espera (sequence_length, batch_size, features)\n",
    "        src = src.permute(1, 0, 2)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        # Volver a (batch_size, sequence_length, features)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        # Promedio sobre la secuencia para obtener (batch_size, features)\n",
    "        output = output.mean(dim=1)\n",
    "        output = self.output_proj(output)\n",
    "        return output\n",
    "\n",
    "print(\"✅ Transformer model architecture defined\")\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_process_data():\n",
    "    \"\"\"\n",
    "    Carga y procesa los datos de relés y optimización GA\n",
    "    \"\"\"\n",
    "    print(\"🔄 Loading and processing data...\")\n",
    "    \n",
    "    # Cargar datos originales\n",
    "    with open(RAW_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    # Cargar resultados de optimización GA\n",
    "    with open(OPTIMIZATION_RESULTS_PATH, 'r', encoding='utf-8') as f:\n",
    "        ga_results = json.load(f)\n",
    "    \n",
    "    print(f\"📊 Raw data loaded: {len(raw_data)} relay pairs\")\n",
    "    print(f\"📊 GA results loaded: {len(ga_results['optimization_results'])} scenarios\")\n",
    "    \n",
    "    # Agrupar datos originales por escenario\n",
    "    raw_by_scenario = defaultdict(list)\n",
    "    for entry in raw_data:\n",
    "        raw_by_scenario[entry['scenario_id']].append(entry)\n",
    "    \n",
    "    # Obtener resultados GA por escenario\n",
    "    ga_by_scenario = ga_results['optimization_results']\n",
    "    \n",
    "    # Crear dataset de entrenamiento\n",
    "    training_data = []\n",
    "    \n",
    "    for scenario_id in ga_by_scenario.keys():\n",
    "        if scenario_id not in raw_by_scenario:\n",
    "            continue\n",
    "            \n",
    "        scenario_raw = raw_by_scenario[scenario_id]\n",
    "        scenario_ga = ga_by_scenario[scenario_id]\n",
    "        \n",
    "        optimized_relays = scenario_ga['relay_values']\n",
    "        \n",
    "        for relay_pair in scenario_raw:\n",
    "            main_relay_id = relay_pair['main_relay']['relay']\n",
    "            backup_relay_id = relay_pair['backup_relay']['relay']\n",
    "            \n",
    "            # Verificar si ambos relés fueron optimizados\n",
    "            if main_relay_id in optimized_relays and backup_relay_id in optimized_relays:\n",
    "                \n",
    "                # Características de entrada\n",
    "                input_features = [\n",
    "                    float(relay_pair['fault']),\n",
    "                    relay_pair['main_relay']['Ishc'],\n",
    "                    relay_pair['main_relay']['Time_out'],\n",
    "                    relay_pair['backup_relay']['Ishc'],\n",
    "                    relay_pair['backup_relay']['Time_out'],\n",
    "                    len(scenario_raw)\n",
    "                ]\n",
    "                \n",
    "                # Características objetivo (valores optimizados por GA)\n",
    "                target_features = [\n",
    "                    optimized_relays[main_relay_id]['TDS'],\n",
    "                    optimized_relays[main_relay_id]['pickup'],\n",
    "                    optimized_relays[backup_relay_id]['TDS'],\n",
    "                    optimized_relays[backup_relay_id]['pickup']\n",
    "                ]\n",
    "                \n",
    "                training_data.append({\n",
    "                    'input': input_features,\n",
    "                    'target': target_features,\n",
    "                    'scenario_id': scenario_id,\n",
    "                    'main_relay': main_relay_id,\n",
    "                    'backup_relay': backup_relay_id\n",
    "                })\n",
    "    \n",
    "    print(f\"📊 Training dataset created: {len(training_data)} samples\")\n",
    "    print(f\"📊 Scenarios included: {len(set(d['scenario_id'] for d in training_data))}\")\n",
    "    \n",
    "    if len(training_data) == 0:\n",
    "        raise ValueError(\"No training data created. Check data files.\")\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "# Cargar datos\n",
    "training_data = load_and_process_data()\n",
    "\n",
    "# =============================================================================\n",
    "# DATA PREPARATION FOR TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_training_data(training_data):\n",
    "    \"\"\"\n",
    "    Prepara los datos para el entrenamiento\n",
    "    \"\"\"\n",
    "    print(\"🔄 Preparing training data...\")\n",
    "    \n",
    "    # Extraer características de entrada y objetivo\n",
    "    X = np.array([item['input'] for item in training_data])\n",
    "    y = np.array([item['target'] for item in training_data])\n",
    "    \n",
    "    # Dividir en entrenamiento y validación\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Normalizar características de entrada\n",
    "    scaler_input = StandardScaler()\n",
    "    X_train_scaled = scaler_input.fit_transform(X_train)\n",
    "    X_val_scaled = scaler_input.transform(X_val)\n",
    "    \n",
    "    # Normalizar características objetivo\n",
    "    scaler_target = StandardScaler()\n",
    "    y_train_scaled = scaler_target.fit_transform(y_train)\n",
    "    y_val_scaled = scaler_target.transform(y_val)\n",
    "    \n",
    "    print(f\"📊 Training samples: {len(X_train_scaled)}\")\n",
    "    print(f\"📊 Validation samples: {len(X_val_scaled)}\")\n",
    "    print(f\"📊 Input features: {X_train_scaled.shape[1]}\")\n",
    "    print(f\"📊 Output features: {y_train_scaled.shape[1]}\")\n",
    "    \n",
    "    return (X_train_scaled, X_val_scaled, y_train_scaled, y_val_scaled, \n",
    "            scaler_input, scaler_target)\n",
    "\n",
    "# Preparar datos\n",
    "X_train, X_val, y_train, y_val, scaler_input, scaler_target = prepare_training_data(training_data)\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Entrena el modelo por una época\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in dataloader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Valida el modelo por una época\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in dataloader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Función objetivo para Optuna\n",
    "    \"\"\"\n",
    "    # Parámetros del modelo\n",
    "    d_model = trial.suggest_categorical('d_model', [32, 64, 128])\n",
    "    nhead = trial.suggest_categorical('nhead', [4, 8, 16])\n",
    "    num_encoder_layers = trial.suggest_int('num_encoder_layers', 2, 6)\n",
    "    dim_feedforward = trial.suggest_categorical('dim_feedforward', [256, 512, 1024])\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.3)\n",
    "    \n",
    "    # Parámetros de entrenamiento\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-4, log=True)\n",
    "    \n",
    "    # Crear modelo\n",
    "    model = RelayOptimizationTransformer(\n",
    "        input_dim=6,\n",
    "        output_dim=4,\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizador y función de pérdida\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Crear dataloaders\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.float32)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(20):  # Máximo 20 épocas por trial\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "        \n",
    "        # Reportar métricas intermedias\n",
    "        trial.report(val_loss, epoch)\n",
    "        \n",
    "        # Verificar si el trial debe ser podado\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return best_val_loss\n",
    "\n",
    "print(\"✅ Training functions defined\")\n",
    "\n",
    "# =============================================================================\n",
    "# OPTUNA OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"🔄 Starting Optuna optimization...\")\n",
    "\n",
    "# Crear estudio Optuna\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    ")\n",
    "\n",
    "# Ejecutar optimización\n",
    "study.optimize(objective, n_trials=20, timeout=1800)  # 20 trials, 30 min timeout\n",
    "\n",
    "print(f\"✅ Optuna optimization completed!\")\n",
    "print(f\"📊 Best trial: {study.best_trial.number}\")\n",
    "print(f\"📊 Best validation loss: {study.best_value:.6f}\")\n",
    "print(f\"📊 Best parameters: {study.best_params}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL MODEL TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"🔄 Training final model with best parameters...\")\n",
    "\n",
    "# Obtener mejores parámetros\n",
    "best_params = study.best_params\n",
    "\n",
    "# Crear modelo final\n",
    "final_model = RelayOptimizationTransformer(\n",
    "    input_dim=6,\n",
    "    output_dim=4,\n",
    "    d_model=best_params['d_model'],\n",
    "    nhead=best_params['nhead'],\n",
    "    num_encoder_layers=best_params['num_encoder_layers'],\n",
    "    dim_feedforward=best_params['dim_feedforward'],\n",
    "    dropout=best_params['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Optimizador y función de pérdida\n",
    "optimizer = optim.Adam(\n",
    "    final_model.parameters(), \n",
    "    lr=best_params['learning_rate'], \n",
    "    weight_decay=best_params['weight_decay']\n",
    ")\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Crear dataloaders\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.float32)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.tensor(X_val, dtype=torch.float32),\n",
    "    torch.tensor(y_val, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "# Entrenar modelo final\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"🔄 Training final model...\")\n",
    "\n",
    "for epoch in range(50):  # Máximo 50 épocas\n",
    "    train_loss = train_epoch(final_model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = validate_epoch(final_model, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"   Epoch {epoch:3d}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Guardar mejor modelo\n",
    "        torch.save(final_model.state_dict(), MODEL_PATH)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"   Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "print(f\"✅ Final model training completed!\")\n",
    "print(f\"📊 Best validation loss: {best_val_loss:.6f}\")\n",
    "print(f\"📊 Total epochs: {len(train_losses)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE MODEL AND ARTIFACTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"🔄 Saving model and artifacts...\")\n",
    "\n",
    "# Guardar scalers\n",
    "with open(SCALER_INPUT_PATH, 'wb') as f:\n",
    "    pickle.dump(scaler_input, f)\n",
    "\n",
    "with open(SCALER_TARGET_PATH, 'wb') as f:\n",
    "    pickle.dump(scaler_target, f)\n",
    "\n",
    "# Guardar mejores parámetros\n",
    "with open(BEST_PARAMS_PATH, 'w') as f:\n",
    "    json.dump(best_params, f, indent=4)\n",
    "\n",
    "# Guardar resumen de entrenamiento\n",
    "training_summary = {\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'best_params': best_params,\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'num_final_epochs': len(train_losses),\n",
    "    'training_date': time.strftime('%Y-%m-%dT%H:%M:%S.%f'),\n",
    "    'mode': 'SINGLE_CELL_EXECUTION'\n",
    "}\n",
    "\n",
    "with open(TRAINING_SUMMARY_PATH, 'w') as f:\n",
    "    json.dump(training_summary, f, indent=4)\n",
    "\n",
    "print(f\"✅ Model and artifacts saved!\")\n",
    "print(f\"📁 Model file: {MODEL_PATH}\")\n",
    "print(f\"📁 Scaler input: {SCALER_INPUT_PATH}\")\n",
    "print(f\"📁 Scaler target: {SCALER_TARGET_PATH}\")\n",
    "print(f\"📁 Best params: {BEST_PARAMS_PATH}\")\n",
    "print(f\"📁 Training summary: {TRAINING_SUMMARY_PATH}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"🔄 Evaluating final model...\")\n",
    "\n",
    "# Cargar mejor modelo\n",
    "final_model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "final_model.eval()\n",
    "\n",
    "# Evaluar en datos de validación\n",
    "with torch.no_grad():\n",
    "    val_predictions = final_model(torch.tensor(X_val, dtype=torch.float32).to(device))\n",
    "    val_predictions = val_predictions.cpu().numpy()\n",
    "    \n",
    "    # Desnormalizar predicciones\n",
    "    val_predictions_denorm = scaler_target.inverse_transform(val_predictions)\n",
    "    val_targets_denorm = scaler_target.inverse_transform(y_val)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    \n",
    "    mse = mean_squared_error(val_targets_denorm, val_predictions_denorm)\n",
    "    mae = mean_absolute_error(val_targets_denorm, val_predictions_denorm)\n",
    "    r2 = r2_score(val_targets_denorm, val_predictions_denorm)\n",
    "    \n",
    "    print(f\"📊 Final Model Performance:\")\n",
    "    print(f\"   • MSE: {mse:.6f}\")\n",
    "    print(f\"   • MAE: {mae:.6f}\")\n",
    "    print(f\"   • R²: {r2:.6f}\")\n",
    "    print(f\"   • RMSE: {np.sqrt(mse):.6f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRANSFORMER PREDICTOR CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class RelayOptimizationPredictor:\n",
    "    \"\"\"\n",
    "    Clase para hacer predicciones con el modelo entrenado\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, scaler_input_path, scaler_target_path, best_params_path):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Cargar parámetros\n",
    "        with open(best_params_path, 'r') as f:\n",
    "            self.best_params = json.load(f)\n",
    "        \n",
    "        # Cargar scalers\n",
    "        with open(scaler_input_path, 'rb') as f:\n",
    "            self.scaler_input = pickle.load(f)\n",
    "        with open(scaler_target_path, 'rb') as f:\n",
    "            self.scaler_target = pickle.load(f)\n",
    "        \n",
    "        # Crear modelo\n",
    "        self.model = RelayOptimizationTransformer(\n",
    "            input_dim=6,\n",
    "            output_dim=4,\n",
    "            d_model=self.best_params['d_model'],\n",
    "            nhead=self.best_params['nhead'],\n",
    "            num_encoder_layers=self.best_params['num_encoder_layers'],\n",
    "            dim_feedforward=self.best_params['dim_feedforward'],\n",
    "            dropout=self.best_params['dropout']\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Cargar pesos\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict_optimization(self, relay_data):\n",
    "        \"\"\"\n",
    "        Predice valores óptimos para un par de relés\n",
    "        \n",
    "        Args:\n",
    "            relay_data: Lista de diccionarios con datos de relés\n",
    "        \n",
    "        Returns:\n",
    "            Lista de predicciones con valores óptimos\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for relay_pair in relay_data:\n",
    "                # Preparar características de entrada\n",
    "                input_features = [\n",
    "                    float(relay_pair['fault']),\n",
    "                    relay_pair['main_relay']['Ishc'],\n",
    "                    relay_pair['main_relay']['Time_out'],\n",
    "                    relay_pair['backup_relay']['Ishc'],\n",
    "                    relay_pair['backup_relay']['Time_out'],\n",
    "                    len(relay_data)\n",
    "                ]\n",
    "                \n",
    "                # Normalizar entrada\n",
    "                input_normalized = self.scaler_input.transform([input_features])\n",
    "                \n",
    "                # Convertir a tensor\n",
    "                input_tensor = torch.tensor(input_normalized, dtype=torch.float32).to(self.device)\n",
    "                \n",
    "                # Hacer predicción\n",
    "                prediction = self.model(input_tensor)\n",
    "                prediction_np = prediction.cpu().numpy().reshape(-1, 4)[0]\n",
    "                \n",
    "                # Desnormalizar predicción\n",
    "                prediction_denorm = self.scaler_target.inverse_transform([prediction_np])[0]\n",
    "                \n",
    "                # Crear resultado\n",
    "                result = {\n",
    "                    'main_relay': {\n",
    "                        'relay': relay_pair['main_relay']['relay'],\n",
    "                        'TDS': max(0.05, min(0.8, prediction_denorm[0])),\n",
    "                        'pickup': max(0.05, min(2.0, prediction_denorm[1]))\n",
    "                    },\n",
    "                    'backup_relay': {\n",
    "                        'relay': relay_pair['backup_relay']['relay'],\n",
    "                        'TDS': max(0.05, min(0.8, prediction_denorm[2])),\n",
    "                        'pickup': max(0.05, min(2.0, prediction_denorm[3]))\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                predictions.append(result)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Guardar clase predictor\n",
    "predictor_code = f\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class RelayOptimizationTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(RelayOptimizationTransformer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.output_proj = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.input_proj.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output_proj.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Asegurar que src tenga 3 dimensiones (batch_size, sequence_length, features)\n",
    "        if src.dim() == 2:\n",
    "            # Si src tiene 2 dimensiones, agregar una dimensión de secuencia\n",
    "            src = src.unsqueeze(1)  # (batch_size, 1, features)\n",
    "        \n",
    "        src = self.input_proj(src) * math.sqrt(self.d_model)\n",
    "        # Transformer espera (sequence_length, batch_size, features)\n",
    "        src = src.permute(1, 0, 2)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        # Volver a (batch_size, sequence_length, features)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        # Promedio sobre la secuencia para obtener (batch_size, features)\n",
    "        output = output.mean(dim=1)\n",
    "        output = self.output_proj(output)\n",
    "        return output\n",
    "\n",
    "class RelayOptimizationPredictor:\n",
    "    def __init__(self, model_path, scaler_input_path, scaler_target_path, best_params_path):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Cargar parámetros\n",
    "        with open(best_params_path, 'r') as f:\n",
    "            self.best_params = json.load(f)\n",
    "        \n",
    "        # Cargar scalers\n",
    "        with open(scaler_input_path, 'rb') as f:\n",
    "            self.scaler_input = pickle.load(f)\n",
    "        with open(scaler_target_path, 'rb') as f:\n",
    "            self.scaler_target = pickle.load(f)\n",
    "        \n",
    "        # Crear modelo\n",
    "        self.model = RelayOptimizationTransformer(\n",
    "            input_dim=6,\n",
    "            output_dim=4,\n",
    "            d_model=self.best_params['d_model'],\n",
    "            nhead=self.best_params['nhead'],\n",
    "            num_encoder_layers=self.best_params['num_encoder_layers'],\n",
    "            dim_feedforward=self.best_params['dim_feedforward'],\n",
    "            dropout=self.best_params['dropout']\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Cargar pesos\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict_optimization(self, relay_data):\n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for relay_pair in relay_data:\n",
    "                # Preparar características de entrada\n",
    "                input_features = [\n",
    "                    float(relay_pair['fault']),\n",
    "                    relay_pair['main_relay']['Ishc'],\n",
    "                    relay_pair['main_relay']['Time_out'],\n",
    "                    relay_pair['backup_relay']['Ishc'],\n",
    "                    relay_pair['backup_relay']['Time_out'],\n",
    "                    len(relay_data)\n",
    "                ]\n",
    "                \n",
    "                # Normalizar entrada\n",
    "                input_normalized = self.scaler_input.transform([input_features])\n",
    "                \n",
    "                # Convertir a tensor\n",
    "                input_tensor = torch.tensor(input_normalized, dtype=torch.float32).to(self.device)\n",
    "                \n",
    "                # Hacer predicción\n",
    "                prediction = self.model(input_tensor)\n",
    "                prediction_np = prediction.cpu().numpy().reshape(-1, 4)[0]\n",
    "                \n",
    "                # Desnormalizar predicción\n",
    "                prediction_denorm = self.scaler_target.inverse_transform([prediction_np])[0]\n",
    "                \n",
    "                # Crear resultado\n",
    "                result = {{\n",
    "                    'main_relay': {{\n",
    "                        'relay': relay_pair['main_relay']['relay'],\n",
    "                        'TDS': max(0.05, min(0.8, prediction_denorm[0])),\n",
    "                        'pickup': max(0.05, min(2.0, prediction_denorm[1]))\n",
    "                    }},\n",
    "                    'backup_relay': {{\n",
    "                        'relay': relay_pair['backup_relay']['relay'],\n",
    "                        'TDS': max(0.05, min(0.8, prediction_denorm[2])),\n",
    "                        'pickup': max(0.05, min(2.0, prediction_denorm[3]))\n",
    "                    }}\n",
    "                }}\n",
    "                \n",
    "                predictions.append(result)\n",
    "        \n",
    "        return predictions\n",
    "\"\"\"\n",
    "\n",
    "# Guardar archivo predictor\n",
    "predictor_path = MODEL_DIR / \"transformer_predictor.py\"\n",
    "with open(predictor_path, 'w') as f:\n",
    "    f.write(predictor_code)\n",
    "\n",
    "print(f\"✅ Predictor class saved: {predictor_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXECUTION COMPLETED\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎉 TRANSFORMER TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n📊 TRAINING SUMMARY:\")\n",
    "print(f\"   • Total training samples: {len(X_train)}\")\n",
    "print(f\"   • Total validation samples: {len(X_val)}\")\n",
    "print(f\"   • Best validation loss: {best_val_loss:.6f}\")\n",
    "print(f\"   • Final R² score: {r2:.6f}\")\n",
    "print(f\"   • Training epochs: {len(train_losses)}\")\n",
    "\n",
    "print(f\"\\n📁 FILES CREATED:\")\n",
    "print(f\"   • Model: {MODEL_PATH}\")\n",
    "print(f\"   • Input scaler: {SCALER_INPUT_PATH}\")\n",
    "print(f\"   • Target scaler: {SCALER_TARGET_PATH}\")\n",
    "print(f\"   • Best parameters: {BEST_PARAMS_PATH}\")\n",
    "print(f\"   • Training summary: {TRAINING_SUMMARY_PATH}\")\n",
    "print(f\"   • Predictor class: {predictor_path}\")\n",
    "\n",
    "print(f\"\\n🚀 NEXT STEPS:\")\n",
    "print(f\"   1. Run the validation notebook to test the model\")\n",
    "print(f\"   2. Use the predictor class for new relay optimizations\")\n",
    "print(f\"   3. Deploy the model for production use\")\n",
    "\n",
    "print(f\"\\n✅ All operations completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
